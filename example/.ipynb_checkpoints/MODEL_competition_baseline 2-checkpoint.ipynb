{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Notebook - DataScience Competition Baseline\n",
    "\n",
    "### Created by Anis Ayari : https://github.com/anisayari on May 2019\n",
    "\n",
    "Please consider to report any enhancements/bug/modification/use to : aayari@deloitte.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS & Math\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#Vizu libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.decomposition import TruncatedSVD,NMF\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, OneHotEncoder\n",
    "from sklearn.model_selection import KFold,cross_val_score,cross_val_predict, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Other ML libraries\n",
    "import featuretools as ft\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from stop_words import get_stop_words\n",
    "stop_words_fr = get_stop_words('fr')\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "#Others\n",
    "import warnings\n",
    "import csv \n",
    "import os \n",
    "import time \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FEATURE ENGINEERING COMMON FUNCTIONS\n",
    "\"\"\"\n",
    "#@TODO : 'Need to check with auto FE libraries\n",
    "\"\"\"\n",
    "MATHEMATICS FEATURES\n",
    "\"\"\"\n",
    "def create_mathematics_features(df, column_to_count, column_to_groupby):\n",
    "    df_tmp = df.groupby(column_to_groupby)[column_to_count].agg(['count','mean', 'std', 'max', 'min'])\n",
    "    df_tmp.columns =['count_' + column_to_count, 'mean_' + column_to_count, 'std_' + column_to_count,'max_' + column_to_count, 'min_' +column_to_count,]\n",
    "    df = df.merge(df_tmp, on=column_to_groupby, how='left')\n",
    "    return df \n",
    "\n",
    "\"\"\"\n",
    "NUMERICAL FEATURES\n",
    "\"\"\"\n",
    "def get_len_columns(df, len_columns):\n",
    "    for col_ in len_columns:\n",
    "        df[\"len_\" + col_] = df[col_].str.len()\n",
    "    return df\n",
    "\n",
    "def transform_to_log(df,columns_to_log):\n",
    "    for col_ in columns_to_log:\n",
    "        df['log_' + col_] = (1+df[col_]).apply(np.log)\n",
    "    return df\n",
    "\n",
    "def count_product_per_store(df, column_to_groupby, column_to_count):\n",
    "    tmp = df.groupby(column_to_groupby).count()[column_to_count].reset_index()\n",
    "    tmp.columns = [column_to_groupby] + [\"number_\" + column_to_count + '_' + column_to_groupby]\n",
    "    df = df.merge(tmp, on=column_to_groupby, how='left')\n",
    "    return df\n",
    "\n",
    "def count_item_column(df, column_to_count, column_groupby):\n",
    "    rescuer_count = df.groupby([column_to_count])[column_groupby].count().reset_index()\n",
    "    rescuer_count.rename(columns={rescuer_count.columns[0]: column_to_count}, inplace=True)\n",
    "    rescuer_count.columns = [column_to_count, column_to_count+'_COUNT']\n",
    "    df = df.merge(rescuer_count, how='left', on=column_to_count)\n",
    "    return df\n",
    "\n",
    "def label_encoding(df,columns_to_encode):\n",
    "    labelencoder = LabelEncoder()\n",
    "    categ_cols = columns_to_encode\n",
    "    for columns_ in categ_cols:\n",
    "        df[columns_+'_ENCODED'] = labelencoder.fit_transform(df[columns_].values.astype(str))\n",
    "    return df\n",
    "\n",
    "def binarie_fill(df,column):\n",
    "    df[column] = df[column].fillna(0)\n",
    "    if True in df[column].tolist():\n",
    "        df[column]= np.where(df[column]==True,1,0)\n",
    "    else:\n",
    "        df[column]= np.where(df[column]==0,0,1)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "TEXT\n",
    "\"\"\"\n",
    "\n",
    "def apply_tfidf_vectorizer(df, column):\n",
    "    df[column] = df[column].fillna(\"missing\")\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), stop_words = stop_words_fr, lowercase=True, \n",
    "                                     max_features=50, binary=True, norm=None,use_idf=False)\n",
    "    tfidf = vectorizer.fit_transform(df[column])\n",
    "    tfidf_cols = vectorizer.get_feature_names()\n",
    "    tmp = pd.DataFrame(data=tfidf.toarray(), columns=['tfidf_' + column + '_' + i for i in tfidf_cols])\n",
    "    df = pd.concat([df, tmp], axis=1,sort=False)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "IMAGE\n",
    "\"\"\"\n",
    "#@TODO : 'To fill'\n",
    "\n",
    "\"\"\"\n",
    "SONG\n",
    "\"\"\"\n",
    "#@TODO : 'To fill'\n",
    "\n",
    "\n",
    "def tfidf_nmf_svd(df,text_columns):\n",
    "    for col_ in tqdm(text_columns):\n",
    "        text = df[col_].values.tolist()\n",
    "        print('[INFO] Start count vectorize')\n",
    "        cvec = CountVectorizer(min_df=2, ngram_range=(1, 3), max_features=1000,\n",
    "                               strip_accents='unicode',\n",
    "                               lowercase=True, analyzer='word', token_pattern=r'\\w+',\n",
    "                               stop_words=stop_words_fr)\n",
    "        text = [str(element) for element in text]\n",
    "        cvec.fit(text)\n",
    "        X = cvec.transform(text)\n",
    "        df['cvec_sum'] = X.sum(axis=1)\n",
    "        df['cvec_mean'] = X.mean(axis=1)\n",
    "        df['cvec_len'] = (X != 0).sum(axis=1)\n",
    "\n",
    "        print('[INFO] Start TFDIDF')\n",
    "        tfv = TfidfVectorizer(min_df=2, max_features=1000,\n",
    "                              strip_accents='unicode', analyzer='word',\n",
    "                              ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                              stop_words=stop_words_fr)\n",
    "\n",
    "        # Fit TFIDF\n",
    "        X = tfv.fit_transform(text)\n",
    "        df['tfidf_sum'] = X.sum(axis=1)\n",
    "        df['tfidf_mean'] = X.mean(axis=1)\n",
    "        df['tfidf_len'] = (X != 0).sum(axis=1)\n",
    "        n_components = 20\n",
    "\n",
    "        print('[INFO] Start NMF')\n",
    "\n",
    "        nmf_ = NMF(n_components=n_components)\n",
    "        X_nmf = nmf_.fit_transform(X)\n",
    "        X_nmf = pd.DataFrame(X_nmf, columns=['{}_nmf_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_nmf['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_nmf.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "\n",
    "        print('[INFO] Start SVD')\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        svd.fit(X)\n",
    "        print('fit done')\n",
    "        X_svd = svd.transform(X)\n",
    "        X_svd = pd.DataFrame(X_svd, columns=['{}_svd_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_svd['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_svd.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "        df.drop(col_, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def auto_features(df):\n",
    "    print('[INFO] Auto Features Processing')\n",
    "    \n",
    "    es = ft.EntitySet(id = 'emmaus')\n",
    "    #es = es.entity_from_dataframe(entity_id = 'data',dataframe = train_test.reset_index(drop=True),make_index = True,index='id')\n",
    "    es = es.entity_from_dataframe(entity_id='data', index='id', dataframe = df)\n",
    "\n",
    "    for groupby in ['brand','category','store_name','product_name','material']:\n",
    "        es = es.normalize_entity(base_entity_id='data', new_entity_id=groupby, index=groupby)\n",
    "    \n",
    "    features, feature_names = ft.dfs(entityset = es, target_entity = 'data', max_depth = 2, verbose=2, n_jobs=5)\n",
    "\n",
    "    df = df.set_index('id').append([fp], sort=False)\n",
    "    return df,feature_names\n",
    "\n",
    "def drop_higlhy_correlated_features(df):\n",
    "    # Threshold for removing correlated variables\n",
    "    threshold = 0.95\n",
    "\n",
    "    # Absolute value correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    upper.head(50)\n",
    "\n",
    "    # Select columns with correlations above threshold\n",
    "    collinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    print('There are %d features to remove.' % (len(collinear_features)))\n",
    "\n",
    "    features_filtered = df.drop(columns = collinear_features)\n",
    "\n",
    "    print('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\n",
    "    features_positive = features_filtered.loc[:, features_filtered.all()]\n",
    "    return features_positive,features_filtered\n",
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    text_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "    df[text_columns] = df[text_columns].fillna('missing')\n",
    "    \n",
    "    df,features_filtered = auto_features(df)\n",
    "    df = df.reset_index()\n",
    "    \"\"\"\n",
    "    DROP NOT RELEVANT COLUMN \n",
    "    \"\"\"\n",
    "    print('[INFO] Dropping Columns...')\n",
    "    columns_to_drop = [\"image_url\", \"sub_category_3\", \"sub_category_4\"]  #'To fill'\n",
    "    df.drop(columns_to_drop, axis = 1, inplace = True)    \n",
    "    \"\"\"\n",
    "    TEXT FEATURES\n",
    "    \"\"\"\n",
    "    print('[INFO] Text Features processing')\n",
    "    \n",
    "    df = get_len_columns(df, len_columns=['product_description'])\n",
    "    \n",
    "    df = label_encoding(df, columns_to_encode=['color','age','product_size',\"brand\",\"shoe_size\"] )\n",
    "        \n",
    "    #count_column = [\"brand\", \"author\", \"editor\"]  #'To fill'\n",
    "    #for col_ in count_column:\n",
    "        #df = count_item_column(df, col_, 'id')\n",
    "    \n",
    "    column_to_vectorize = [\"sub_category_1\", \"sub_category_2\",'store_name','product_description',\n",
    "                    'material', 'editor', 'product_name',\"author\"]  #'To fill'\n",
    "    \n",
    "    #for column_ in column_to_vectorize:\n",
    "        #if column_ in df.columns :\n",
    "            #df=apply_tfidf_vectorizer(df,column_)\n",
    "            #df.drop(column_, inplace=True, axis=1)\n",
    "    df=tfidf_nmf_svd(df,text_columns=column_to_vectorize)\n",
    "    \n",
    "    binary_column = ['warranty','wifi','vintage']  #'To fill'\n",
    "    for col_ in binary_column:\n",
    "        df = binarie_fill(df,col_)\n",
    "    \n",
    "    columns_to_dummies = ['category']  # 'To fill'\n",
    "    for col_ in columns_to_dummies:\n",
    "        df = pd.concat([df.drop(col_, axis=1), pd.get_dummies(df[col_],prefix=col_)], axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    NUMERICAL FEATURES\n",
    "    \"\"\"\n",
    "    column_to_count = 'price'    #'To fill'\n",
    "    column_to_groupby = 'store_name'    #'To fill'\n",
    "    #df = create_mathematics_features(df, column_to_count, column_to_groupby)\n",
    "    \n",
    "    \n",
    "    columns_to_log = [\"price\", \"len_product_description\"]  #'To fill'\n",
    "    transform_to_log(df,columns_to_log)\n",
    "\n",
    "    #to_drop = [\"price\",\"id\",'image_width','image_height','color','age','product_size',\"brand\",\"shoe_size\",\"len_product_description\", \"condition\", \"year\", \"product_width\",\"product_length\", \"product_height\"]  #'To fill'\n",
    "    #df.drop(to_drop,inplace=True, axis=1)\n",
    "    df,features_filtered=drop_higlhy_correlated_features(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2168: expected 31 fields, saw 33\\nSkipping line 4822: expected 31 fields, saw 37\\nSkipping line 4859: expected 31 fields, saw 37\\nSkipping line 7342: expected 31 fields, saw 37\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Auto Features Processing\n",
      "Built 507 features\n",
      "EntitySet scattered to 1 workers in 1 seconds\n",
      "\n",
      "\n",
      "\n",
      "Elapsed: 00:00 | Remaining: ? | Progress:   0%|                                                | Calculated: 0/1 chunks\n",
      "\n",
      "\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|████████████████████████████████████████████| Calculated: 1/1 chunks[INFO] Dropping Columns...\n",
      "[INFO] Text Features processing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8dc33f09d940c2909db8a8a73fdd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start NMF\n",
      "[INFO] Start SVD\n",
      "fit done\n",
      "There are 151 features to remove.\n",
      "The number of features that passed the collinearity threshold:  377\n",
      "               id  images_count  image_width  image_height product_size  \\\n",
      "0      5142_train           2.0       1100.0        1100.0      missing   \n",
      "1      3110_train           3.0       2200.0        2200.0      missing   \n",
      "2       703_train           4.0       1601.0        1725.0      missing   \n",
      "3      1595_train           1.0        600.0         378.0      missing   \n",
      "4      3952_train           1.0       2160.0        2160.0      missing   \n",
      "5      7384_train           1.0        600.0         366.0      missing   \n",
      "6      3351_train           2.0       3072.0        4608.0      missing   \n",
      "7       596_train           3.0       1536.0        1536.0            L   \n",
      "8       2662_test           6.0        800.0         800.0      missing   \n",
      "9      1002_train           3.0        837.0        1024.0      missing   \n",
      "10         0_test           NaN        616.0         616.0          NaN   \n",
      "11        0_train           NaN       3458.0        2552.0           44   \n",
      "12      1000_test           NaN       1100.0        1100.0          NaN   \n",
      "13     1000_train           NaN       1536.0        1536.0          NaN   \n",
      "14      1001_test           NaN        656.0        1024.0          NaN   \n",
      "15     1001_train           NaN       3072.0        4608.0          NaN   \n",
      "16      1002_test           NaN       2402.0        2041.0          NaN   \n",
      "17     1002_train           NaN        837.0        1024.0          NaN   \n",
      "18      1003_test           NaN        616.0         616.0          NaN   \n",
      "19     1003_train           NaN       1100.0        1100.0          NaN   \n",
      "20      1004_test           NaN       3239.0        3239.0          NaN   \n",
      "21     1004_train           NaN       2226.0        2730.0          NaN   \n",
      "22      1005_test           NaN        405.0         250.0          NaN   \n",
      "23     1005_train           NaN       1536.0        1536.0          NaN   \n",
      "24      1006_test           NaN        616.0         616.0          NaN   \n",
      "25     1006_train           NaN       2448.0        2448.0           42   \n",
      "26      1007_test           NaN       1536.0        1536.0          NaN   \n",
      "27     1007_train           NaN       1536.0        1536.0           38   \n",
      "28      1008_test           NaN       2234.0        1160.0           44   \n",
      "29     1008_train           NaN       1100.0        1100.0          NaN   \n",
      "...           ...           ...          ...           ...          ...   \n",
      "11820    988_test           NaN        209.0         250.0          NaN   \n",
      "11821   988_train           NaN       2940.0        2940.0          NaN   \n",
      "11822    989_test           NaN        616.0         616.0          NaN   \n",
      "11823   989_train           NaN        900.0         675.0          NaN   \n",
      "11824     98_test           NaN       1010.0        1364.0          NaN   \n",
      "11825    98_train           NaN       4608.0        3072.0          NaN   \n",
      "11826    990_test           NaN       1536.0        1536.0          NaN   \n",
      "11827   990_train           NaN        900.0         675.0           40   \n",
      "11828    991_test           NaN       3024.0        3024.0          NaN   \n",
      "11829   991_train           NaN       2664.0        2664.0          NaN   \n",
      "11830    992_test           NaN       1098.0        1100.0          NaN   \n",
      "11831   992_train           NaN       1536.0        1536.0          NaN   \n",
      "11832    993_test           NaN       1100.0        1100.0          NaN   \n",
      "11833   993_train           NaN       3024.0        3024.0          NaN   \n",
      "11834    994_test           NaN       1080.0        1920.0          NaN   \n",
      "11835   994_train           NaN       3024.0        3024.0          NaN   \n",
      "11836    995_test           NaN        431.0         461.0          NaN   \n",
      "11837   995_train           NaN       3024.0        3024.0          NaN   \n",
      "11838    996_test           NaN       3000.0        4000.0          NaN   \n",
      "11839   996_train           NaN       3000.0        4000.0          NaN   \n",
      "11840    997_test           NaN       1536.0        1536.0           40   \n",
      "11841   997_train           NaN       2448.0        2448.0           36   \n",
      "11842    998_test           NaN       1000.0         890.0           44   \n",
      "11843   998_train           NaN       1000.0         560.0           36   \n",
      "11844    999_test           NaN       2304.0        3456.0          NaN   \n",
      "11845   999_train           NaN       3072.0        4608.0          NaN   \n",
      "11846     99_test           NaN       1536.0        1536.0          NaN   \n",
      "11847    99_train           NaN       3674.0        2264.0            M   \n",
      "11848      9_test           NaN       4608.0        2720.0          NaN   \n",
      "11849     9_train           NaN       2448.0        2448.0           38   \n",
      "\n",
      "           age  warranty        color  product_width  wifi  ...  \\\n",
      "0      missing         1      missing            NaN     1  ...   \n",
      "1      missing         1      missing            NaN     1  ...   \n",
      "2      missing         1       Violet            NaN     1  ...   \n",
      "3      missing         1      missing            NaN     1  ...   \n",
      "4      missing         1      missing            NaN     1  ...   \n",
      "5      missing         1      missing            NaN     1  ...   \n",
      "6      missing         1      missing            NaN     1  ...   \n",
      "7      missing         1         Bleu            NaN     1  ...   \n",
      "8      missing         1         Noir            NaN     1  ...   \n",
      "9      missing         1      missing            NaN     1  ...   \n",
      "10         NaN         0          NaN            NaN     0  ...   \n",
      "11         NaN         0  Multicolore            NaN     0  ...   \n",
      "12         NaN         0         Bleu            NaN     0  ...   \n",
      "13         NaN         0         Noir            NaN     0  ...   \n",
      "14         NaN         0          NaN            NaN     0  ...   \n",
      "15         NaN         0         Bleu            NaN     0  ...   \n",
      "16         NaN         0        Rouge            NaN     0  ...   \n",
      "17         NaN         0          NaN            NaN     0  ...   \n",
      "18         NaN         0          NaN            NaN     0  ...   \n",
      "19         NaN         0          NaN            NaN     0  ...   \n",
      "20         NaN         0          NaN            NaN     0  ...   \n",
      "21         NaN         0  Multicolore            NaN     0  ...   \n",
      "22         NaN         0          NaN            NaN     0  ...   \n",
      "23         NaN         0         Noir            NaN     0  ...   \n",
      "24         NaN         0          NaN            NaN     0  ...   \n",
      "25         NaN         0         Noir            NaN     0  ...   \n",
      "26         10a         0       Orange            NaN     0  ...   \n",
      "27         NaN         0         Noir            NaN     0  ...   \n",
      "28         NaN         0         Noir            NaN     0  ...   \n",
      "29         NaN         0  Multicolore            NaN     0  ...   \n",
      "...        ...       ...          ...            ...   ...  ...   \n",
      "11820      NaN         0          NaN            NaN     0  ...   \n",
      "11821      NaN         0         Gris            NaN     0  ...   \n",
      "11822      NaN         0          NaN            NaN     0  ...   \n",
      "11823      NaN         0       Marron            NaN     0  ...   \n",
      "11824      NaN         0        Rouge            NaN     0  ...   \n",
      "11825      NaN         0  Multicolore            NaN     0  ...   \n",
      "11826      NaN         0          NaN            NaN     0  ...   \n",
      "11827      NaN         0          NaN            NaN     0  ...   \n",
      "11828      NaN         0        Blanc            NaN     0  ...   \n",
      "11829      NaN         0          NaN            NaN     0  ...   \n",
      "11830      NaN         0         Bleu            NaN     0  ...   \n",
      "11831      NaN         0         Noir            NaN     0  ...   \n",
      "11832      NaN         0          NaN            NaN     0  ...   \n",
      "11833      NaN         0         Bleu            NaN     0  ...   \n",
      "11834      NaN         0          NaN            NaN     0  ...   \n",
      "11835      NaN         0         Bleu            NaN     0  ...   \n",
      "11836      NaN         0  Multicolore            NaN     0  ...   \n",
      "11837      NaN         0         Noir            NaN     0  ...   \n",
      "11838      NaN         0        Blanc            NaN     0  ...   \n",
      "11839      NaN         0  Multicolore            NaN     0  ...   \n",
      "11840      NaN         0       Marron            NaN     0  ...   \n",
      "11841      NaN         0         Gris            NaN     0  ...   \n",
      "11842      NaN         0  Multicolore            NaN     0  ...   \n",
      "11843      NaN         0  Multicolore            NaN     0  ...   \n",
      "11844      NaN         0  Multicolore            NaN     0  ...   \n",
      "11845      NaN         0       Marron            NaN     0  ...   \n",
      "11846      NaN         0       Marron            NaN     0  ...   \n",
      "11847      NaN         0         Bleu            NaN     0  ...   \n",
      "11848      NaN         0          NaN            NaN     0  ...   \n",
      "11849      NaN         0        Taupe            NaN     0  ...   \n",
      "\n",
      "      category_culture - loisirs  category_enfance  category_label selection  \\\n",
      "0                              0                 0                         0   \n",
      "1                              0                 0                         0   \n",
      "2                              0                 0                         0   \n",
      "3                              0                 0                         0   \n",
      "4                              0                 0                         0   \n",
      "5                              0                 0                         0   \n",
      "6                              0                 0                         0   \n",
      "7                              0                 0                         0   \n",
      "8                              0                 0                         0   \n",
      "9                              0                 0                         0   \n",
      "10                             0                 0                         0   \n",
      "11                             0                 0                         0   \n",
      "12                             0                 0                         0   \n",
      "13                             0                 0                         0   \n",
      "14                             0                 0                         0   \n",
      "15                             0                 0                         0   \n",
      "16                             0                 0                         0   \n",
      "17                             0                 0                         0   \n",
      "18                             0                 0                         0   \n",
      "19                             0                 0                         0   \n",
      "20                             0                 0                         0   \n",
      "21                             0                 0                         0   \n",
      "22                             0                 0                         0   \n",
      "23                             0                 0                         0   \n",
      "24                             0                 0                         0   \n",
      "25                             0                 0                         0   \n",
      "26                             0                 0                         1   \n",
      "27                             0                 0                         0   \n",
      "28                             0                 0                         0   \n",
      "29                             0                 0                         0   \n",
      "...                          ...               ...                       ...   \n",
      "11820                          0                 0                         0   \n",
      "11821                          0                 0                         0   \n",
      "11822                          0                 0                         0   \n",
      "11823                          0                 0                         0   \n",
      "11824                          0                 1                         0   \n",
      "11825                          0                 0                         0   \n",
      "11826                          0                 0                         0   \n",
      "11827                          0                 0                         0   \n",
      "11828                          0                 1                         0   \n",
      "11829                          0                 0                         0   \n",
      "11830                          0                 1                         0   \n",
      "11831                          0                 0                         0   \n",
      "11832                          0                 0                         0   \n",
      "11833                          0                 0                         0   \n",
      "11834                          0                 0                         0   \n",
      "11835                          0                 0                         0   \n",
      "11836                          0                 0                         0   \n",
      "11837                          0                 0                         0   \n",
      "11838                          0                 0                         0   \n",
      "11839                          0                 1                         0   \n",
      "11840                          0                 0                         1   \n",
      "11841                          0                 0                         0   \n",
      "11842                          0                 0                         0   \n",
      "11843                          0                 0                         1   \n",
      "11844                          0                 0                         0   \n",
      "11845                          0                 0                         1   \n",
      "11846                          0                 0                         0   \n",
      "11847                          0                 0                         1   \n",
      "11848                          0                 0                         0   \n",
      "11849                          0                 0                         1   \n",
      "\n",
      "       category_les coups de coeur des vendeurs category_librairie  \\\n",
      "0                                             0                  0   \n",
      "1                                             0                  0   \n",
      "2                                             0                  0   \n",
      "3                                             0                  1   \n",
      "4                                             0                  1   \n",
      "5                                             0                  1   \n",
      "6                                             0                  0   \n",
      "7                                             0                  0   \n",
      "8                                             0                  0   \n",
      "9                                             0                  0   \n",
      "10                                            0                  1   \n",
      "11                                            0                  0   \n",
      "12                                            0                  1   \n",
      "13                                            0                  0   \n",
      "14                                            0                  0   \n",
      "15                                            0                  0   \n",
      "16                                            0                  0   \n",
      "17                                            0                  0   \n",
      "18                                            0                  1   \n",
      "19                                            0                  0   \n",
      "20                                            0                  0   \n",
      "21                                            0                  0   \n",
      "22                                            0                  1   \n",
      "23                                            0                  0   \n",
      "24                                            0                  1   \n",
      "25                                            0                  0   \n",
      "26                                            0                  0   \n",
      "27                                            0                  0   \n",
      "28                                            0                  0   \n",
      "29                                            0                  0   \n",
      "...                                         ...                ...   \n",
      "11820                                         0                  1   \n",
      "11821                                         0                  0   \n",
      "11822                                         0                  1   \n",
      "11823                                         0                  0   \n",
      "11824                                         0                  0   \n",
      "11825                                         0                  0   \n",
      "11826                                         0                  0   \n",
      "11827                                         0                  0   \n",
      "11828                                         0                  0   \n",
      "11829                                         0                  0   \n",
      "11830                                         0                  0   \n",
      "11831                                         0                  0   \n",
      "11832                                         0                  0   \n",
      "11833                                         0                  0   \n",
      "11834                                         0                  0   \n",
      "11835                                         0                  0   \n",
      "11836                                         0                  0   \n",
      "11837                                         0                  0   \n",
      "11838                                         0                  0   \n",
      "11839                                         0                  0   \n",
      "11840                                         0                  0   \n",
      "11841                                         0                  0   \n",
      "11842                                         0                  0   \n",
      "11843                                         0                  0   \n",
      "11844                                         0                  0   \n",
      "11845                                         0                  0   \n",
      "11846                                         0                  0   \n",
      "11847                                         0                  0   \n",
      "11848                                         0                  1   \n",
      "11849                                         0                  0   \n",
      "\n",
      "       category_loisirs  category_mobilier - deco  \\\n",
      "0                     0                         1   \n",
      "1                     0                         0   \n",
      "2                     0                         0   \n",
      "3                     0                         0   \n",
      "4                     0                         0   \n",
      "5                     0                         0   \n",
      "6                     1                         0   \n",
      "7                     0                         0   \n",
      "8                     0                         0   \n",
      "9                     0                         1   \n",
      "10                    0                         0   \n",
      "11                    0                         0   \n",
      "12                    0                         0   \n",
      "13                    0                         0   \n",
      "14                    1                         0   \n",
      "15                    0                         1   \n",
      "16                    0                         0   \n",
      "17                    0                         1   \n",
      "18                    0                         0   \n",
      "19                    1                         0   \n",
      "20                    0                         1   \n",
      "21                    0                         1   \n",
      "22                    0                         0   \n",
      "23                    0                         0   \n",
      "24                    0                         0   \n",
      "25                    0                         0   \n",
      "26                    0                         0   \n",
      "27                    0                         0   \n",
      "28                    0                         0   \n",
      "29                    0                         1   \n",
      "...                 ...                       ...   \n",
      "11820                 0                         0   \n",
      "11821                 0                         1   \n",
      "11822                 0                         0   \n",
      "11823                 0                         0   \n",
      "11824                 0                         0   \n",
      "11825                 0                         1   \n",
      "11826                 0                         1   \n",
      "11827                 0                         0   \n",
      "11828                 0                         0   \n",
      "11829                 0                         1   \n",
      "11830                 0                         0   \n",
      "11831                 0                         0   \n",
      "11832                 0                         1   \n",
      "11833                 0                         0   \n",
      "11834                 1                         0   \n",
      "11835                 0                         1   \n",
      "11836                 0                         0   \n",
      "11837                 1                         0   \n",
      "11838                 0                         1   \n",
      "11839                 0                         0   \n",
      "11840                 0                         0   \n",
      "11841                 0                         0   \n",
      "11842                 0                         0   \n",
      "11843                 0                         0   \n",
      "11844                 0                         1   \n",
      "11845                 0                         0   \n",
      "11846                 0                         0   \n",
      "11847                 0                         0   \n",
      "11848                 0                         0   \n",
      "11849                 0                         0   \n",
      "\n",
      "       category_mobilier - deco - maison  category_mode  category_multimédia  \n",
      "0                                      0              0                    0  \n",
      "1                                      0              1                    0  \n",
      "2                                      0              1                    0  \n",
      "3                                      0              0                    0  \n",
      "4                                      0              0                    0  \n",
      "5                                      0              0                    0  \n",
      "6                                      0              0                    0  \n",
      "7                                      0              1                    0  \n",
      "8                                      0              1                    0  \n",
      "9                                      0              0                    0  \n",
      "10                                     0              0                    0  \n",
      "11                                     0              1                    0  \n",
      "12                                     0              0                    0  \n",
      "13                                     0              1                    0  \n",
      "14                                     0              0                    0  \n",
      "15                                     0              0                    0  \n",
      "16                                     0              1                    0  \n",
      "17                                     0              0                    0  \n",
      "18                                     0              0                    0  \n",
      "19                                     0              0                    0  \n",
      "20                                     0              0                    0  \n",
      "21                                     0              0                    0  \n",
      "22                                     0              0                    0  \n",
      "23                                     0              1                    0  \n",
      "24                                     0              0                    0  \n",
      "25                                     0              1                    0  \n",
      "26                                     0              0                    0  \n",
      "27                                     0              1                    0  \n",
      "28                                     0              1                    0  \n",
      "29                                     0              0                    0  \n",
      "...                                  ...            ...                  ...  \n",
      "11820                                  0              0                    0  \n",
      "11821                                  0              0                    0  \n",
      "11822                                  0              0                    0  \n",
      "11823                                  0              1                    0  \n",
      "11824                                  0              0                    0  \n",
      "11825                                  0              0                    0  \n",
      "11826                                  0              0                    0  \n",
      "11827                                  0              1                    0  \n",
      "11828                                  0              0                    0  \n",
      "11829                                  0              0                    0  \n",
      "11830                                  0              0                    0  \n",
      "11831                                  0              1                    0  \n",
      "11832                                  0              0                    0  \n",
      "11833                                  0              1                    0  \n",
      "11834                                  0              0                    0  \n",
      "11835                                  0              0                    0  \n",
      "11836                                  0              1                    0  \n",
      "11837                                  0              0                    0  \n",
      "11838                                  0              0                    0  \n",
      "11839                                  0              0                    0  \n",
      "11840                                  0              0                    0  \n",
      "11841                                  0              1                    0  \n",
      "11842                                  0              1                    0  \n",
      "11843                                  0              0                    0  \n",
      "11844                                  0              0                    0  \n",
      "11845                                  0              0                    0  \n",
      "11846                                  0              1                    0  \n",
      "11847                                  0              0                    0  \n",
      "11848                                  0              0                    0  \n",
      "11849                                  0              0                    0  \n",
      "\n",
      "[11850 rows x 377 columns]\n",
      "Memory usage of dataframe is 16.36 MB\n",
      "Memory usage after optimization is: 5.47 MB\n",
      "Decreased by 66.5%\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"X_train.csv\", index_col=0, error_bad_lines=False)\n",
    "len_train = len(train)\n",
    "test = pd.read_csv(\"X_test.csv\", index_col=0, error_bad_lines=False)\n",
    "\n",
    "train = train.reset_index()\n",
    "test= test.reset_index()\n",
    "train['id']  = train['id'].astype(str)+'_'+'train'\n",
    "test['id']  = test['id'].astype(str)+'_'+'test'\n",
    "\n",
    "#t\n",
    "y = pd.read_csv(\"y_train.csv\", index_col=0)\n",
    "train_test = pd.concat((train, test), axis=0)\n",
    "train_test = features_engineering(train_test.sample(10))\n",
    "#train_test = train_test.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.reset_index(drop=True)\n",
    "#fp.drop('id', inplace=True, axis=1)\n",
    "train = fp.iloc[:len_train, :]\n",
    "test = fp.iloc[len_train:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Auto Features Processing\n",
      "Built 507 features\n",
      "EntitySet scattered to 5 workers in 5 seconds\n",
      "Elapsed: 00:35 | Remaining: 00:00 | Progress: 100%|██████████████████████████████████████████| Calculated: 10/10 chunks\n",
      "There are 212 features to remove.\n",
      "The number of features that passed the collinearity threshold:  295\n"
     ]
    }
   ],
   "source": [
    "fp = auto_features(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'images_count',\n",
       " 'image_width',\n",
       " 'image_height',\n",
       " 'product_size',\n",
       " 'age',\n",
       " 'color',\n",
       " 'product_width',\n",
       " 'condition',\n",
       " 'product_length',\n",
       " 'shoe_size',\n",
       " 'brand',\n",
       " 'product_height',\n",
       " 'NUM_WORDS(image_url)',\n",
       " 'NUM_WORDS(product_description)',\n",
       " 'brand.STD(data.product_width)',\n",
       " 'brand.STD(data.product_length)',\n",
       " 'brand.STD(data.product_height)',\n",
       " 'brand.MAX(data.image_width)',\n",
       " 'brand.MAX(data.image_height)',\n",
       " 'brand.MAX(data.year)',\n",
       " 'brand.MAX(data.shoe_size)',\n",
       " 'brand.MAX(data.price)',\n",
       " 'brand.SKEW(data.product_width)',\n",
       " 'brand.SKEW(data.product_length)',\n",
       " 'brand.SKEW(data.product_height)',\n",
       " 'brand.MIN(data.shoe_size)',\n",
       " 'brand.MIN(data.weight)',\n",
       " 'brand.MIN(data.price)',\n",
       " 'brand.MEAN(data.image_width)',\n",
       " 'brand.MEAN(data.image_height)',\n",
       " 'brand.MEAN(data.year)',\n",
       " 'brand.MEAN(data.shoe_size)',\n",
       " 'brand.COUNT(data)',\n",
       " 'brand.NUM_UNIQUE(data.store_name)',\n",
       " 'brand.MODE(data.product_size)',\n",
       " 'brand.MODE(data.material)',\n",
       " 'brand.MODE(data.age)',\n",
       " 'brand.MODE(data.warranty)',\n",
       " 'brand.MODE(data.color)',\n",
       " 'brand.MODE(data.condition)',\n",
       " 'brand.MODE(data.author)',\n",
       " 'brand.MODE(data.editor)',\n",
       " 'brand.MODE(data.category)',\n",
       " 'brand.MODE(data.sub_category_1)',\n",
       " 'brand.MODE(data.sub_category_2)',\n",
       " 'brand.MODE(data.sub_category_3)',\n",
       " 'brand.MODE(data.sub_category_4)',\n",
       " 'brand.MODE(data.product_name)',\n",
       " 'brand.MODE(data.store_name)',\n",
       " 'category.SUM(data.images_count)',\n",
       " 'category.SUM(data.price)',\n",
       " 'category.STD(data.images_count)',\n",
       " 'category.STD(data.image_width)',\n",
       " 'category.STD(data.image_height)',\n",
       " 'category.STD(data.shoe_size)',\n",
       " 'category.STD(data.weight)',\n",
       " 'category.MAX(data.images_count)',\n",
       " 'category.MAX(data.image_width)',\n",
       " 'category.MAX(data.image_height)',\n",
       " 'category.MAX(data.product_width)',\n",
       " 'category.MAX(data.shoe_size)',\n",
       " 'category.MAX(data.price)',\n",
       " 'category.SKEW(data.images_count)',\n",
       " 'category.SKEW(data.image_height)',\n",
       " 'category.MIN(data.price)',\n",
       " 'category.NUM_UNIQUE(data.condition)',\n",
       " 'category.NUM_UNIQUE(data.vintage)',\n",
       " 'category.NUM_UNIQUE(data.sub_category_1)',\n",
       " 'category.MODE(data.product_size)',\n",
       " 'category.MODE(data.material)',\n",
       " 'category.MODE(data.age)',\n",
       " 'category.MODE(data.warranty)',\n",
       " 'category.MODE(data.color)',\n",
       " 'category.MODE(data.condition)',\n",
       " 'category.MODE(data.brand)',\n",
       " 'category.MODE(data.author)',\n",
       " 'category.MODE(data.editor)',\n",
       " 'category.MODE(data.sub_category_1)',\n",
       " 'category.MODE(data.sub_category_2)',\n",
       " 'category.MODE(data.sub_category_3)',\n",
       " 'category.MODE(data.sub_category_4)',\n",
       " 'category.MODE(data.product_name)',\n",
       " 'category.MODE(data.store_name)',\n",
       " 'store_name.SUM(data.images_count)',\n",
       " 'store_name.SUM(data.image_width)',\n",
       " 'store_name.SUM(data.weight)',\n",
       " 'store_name.SUM(data.price)',\n",
       " 'store_name.STD(data.product_height)',\n",
       " 'store_name.MAX(data.images_count)',\n",
       " 'store_name.MAX(data.image_width)',\n",
       " 'store_name.MAX(data.image_height)',\n",
       " 'store_name.MAX(data.product_length)',\n",
       " 'store_name.MAX(data.shoe_size)',\n",
       " 'store_name.MAX(data.product_height)',\n",
       " 'store_name.MAX(data.weight)',\n",
       " 'store_name.MAX(data.price)',\n",
       " 'store_name.SKEW(data.year)',\n",
       " 'store_name.MIN(data.image_width)',\n",
       " 'store_name.MIN(data.product_width)',\n",
       " 'store_name.MIN(data.product_length)',\n",
       " 'store_name.MIN(data.weight)',\n",
       " 'store_name.MIN(data.price)',\n",
       " 'store_name.MEAN(data.images_count)',\n",
       " 'store_name.MEAN(data.image_width)',\n",
       " 'store_name.MEAN(data.image_height)',\n",
       " 'store_name.MEAN(data.product_width)',\n",
       " 'store_name.MEAN(data.product_length)',\n",
       " 'store_name.MEAN(data.shoe_size)',\n",
       " 'store_name.MEAN(data.weight)',\n",
       " 'store_name.MEAN(data.price)',\n",
       " 'store_name.COUNT(data)',\n",
       " 'store_name.NUM_UNIQUE(data.condition)',\n",
       " 'store_name.NUM_UNIQUE(data.category)',\n",
       " 'store_name.MODE(data.product_size)',\n",
       " 'store_name.MODE(data.material)',\n",
       " 'store_name.MODE(data.age)',\n",
       " 'store_name.MODE(data.warranty)',\n",
       " 'store_name.MODE(data.color)',\n",
       " 'store_name.MODE(data.condition)',\n",
       " 'store_name.MODE(data.brand)',\n",
       " 'store_name.MODE(data.author)',\n",
       " 'store_name.MODE(data.editor)',\n",
       " 'store_name.MODE(data.category)',\n",
       " 'store_name.MODE(data.sub_category_1)',\n",
       " 'store_name.MODE(data.sub_category_2)',\n",
       " 'store_name.MODE(data.sub_category_3)',\n",
       " 'store_name.MODE(data.sub_category_4)',\n",
       " 'store_name.MODE(data.product_name)',\n",
       " 'product_name.STD(data.shoe_size)',\n",
       " 'product_name.SKEW(data.product_width)',\n",
       " 'product_name.SKEW(data.product_length)',\n",
       " 'product_name.SKEW(data.shoe_size)',\n",
       " 'product_name.SKEW(data.product_height)',\n",
       " 'product_name.MODE(data.product_size)',\n",
       " 'product_name.MODE(data.material)',\n",
       " 'product_name.MODE(data.age)',\n",
       " 'product_name.MODE(data.warranty)',\n",
       " 'product_name.MODE(data.color)',\n",
       " 'product_name.MODE(data.condition)',\n",
       " 'product_name.MODE(data.brand)',\n",
       " 'product_name.MODE(data.author)',\n",
       " 'product_name.MODE(data.editor)',\n",
       " 'product_name.MODE(data.category)',\n",
       " 'product_name.MODE(data.sub_category_1)',\n",
       " 'product_name.MODE(data.sub_category_2)',\n",
       " 'product_name.MODE(data.sub_category_3)',\n",
       " 'product_name.MODE(data.sub_category_4)',\n",
       " 'product_name.MODE(data.store_name)',\n",
       " 'material.STD(data.year)',\n",
       " 'material.MAX(data.image_width)',\n",
       " 'material.MAX(data.image_height)',\n",
       " 'material.MAX(data.price)',\n",
       " 'material.SKEW(data.year)',\n",
       " 'material.MIN(data.image_width)',\n",
       " 'material.MIN(data.image_height)',\n",
       " 'material.MIN(data.shoe_size)',\n",
       " 'material.MIN(data.weight)',\n",
       " 'material.MIN(data.price)',\n",
       " 'material.MEAN(data.image_width)',\n",
       " 'material.MEAN(data.price)',\n",
       " 'material.NUM_UNIQUE(data.store_name)',\n",
       " 'material.MODE(data.product_size)',\n",
       " 'material.MODE(data.age)',\n",
       " 'material.MODE(data.warranty)',\n",
       " 'material.MODE(data.color)',\n",
       " 'material.MODE(data.condition)',\n",
       " 'material.MODE(data.brand)',\n",
       " 'material.MODE(data.author)',\n",
       " 'material.MODE(data.editor)',\n",
       " 'material.MODE(data.category)',\n",
       " 'material.MODE(data.sub_category_1)',\n",
       " 'material.MODE(data.sub_category_2)',\n",
       " 'material.MODE(data.sub_category_3)',\n",
       " 'material.MODE(data.sub_category_4)',\n",
       " 'material.MODE(data.product_name)',\n",
       " 'material.MODE(data.store_name)',\n",
       " 'product_description_svd_7',\n",
       " 'product_description_svd_9',\n",
       " 'product_description_svd_10',\n",
       " 'product_description_svd_11']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_test.iloc[:len_train, :]\n",
    "test = train_test.iloc[len_train:, :]\n",
    "test_id = test.index\n",
    "train['label'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8880, 75)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['images_count', 'image_width', 'image_height', 'year', 'product_width',\n",
       "       'product_length', 'shoe_size', 'product_height', 'price', 'image_width',\n",
       "       'image_height', 'product_width', 'product_length', 'shoe_size',\n",
       "       'product_height', 'price', 'NUM_WORDS(product_description)',\n",
       "       'brand.MAX(data.image_width)', 'brand.MAX(data.image_height)',\n",
       "       'brand.MAX(data.year)', 'brand.MAX(data.shoe_size)',\n",
       "       'brand.MAX(data.price)', 'brand.MIN(data.shoe_size)',\n",
       "       'brand.MIN(data.price)', 'brand.MEAN(data.image_width)',\n",
       "       'brand.MEAN(data.image_height)', 'brand.MEAN(data.year)',\n",
       "       'brand.COUNT(data)', 'brand.NUM_UNIQUE(data.store_name)',\n",
       "       'category.SUM(data.images_count)', 'category.SUM(data.price)',\n",
       "       'category.STD(data.image_width)', 'category.STD(data.image_height)',\n",
       "       'category.MAX(data.images_count)', 'category.MAX(data.image_width)',\n",
       "       'category.MAX(data.image_height)', 'category.MAX(data.product_width)',\n",
       "       'category.MAX(data.shoe_size)', 'category.MAX(data.price)',\n",
       "       'category.NUM_UNIQUE(data.sub_category_1)',\n",
       "       'store_name.SUM(data.images_count)', 'store_name.SUM(data.price)',\n",
       "       'store_name.STD(data.product_height)',\n",
       "       'store_name.MAX(data.images_count)', 'store_name.MAX(data.image_width)',\n",
       "       'store_name.MAX(data.image_height)',\n",
       "       'store_name.MAX(data.product_length)', 'store_name.MAX(data.shoe_size)',\n",
       "       'store_name.MAX(data.product_height)', 'store_name.MAX(data.price)',\n",
       "       'store_name.SKEW(data.year)', 'store_name.MIN(data.image_width)',\n",
       "       'store_name.MIN(data.product_width)',\n",
       "       'store_name.MIN(data.product_length)', 'store_name.MIN(data.weight)',\n",
       "       'store_name.MIN(data.price)', 'store_name.MEAN(data.images_count)',\n",
       "       'store_name.MEAN(data.image_width)',\n",
       "       'store_name.MEAN(data.image_height)',\n",
       "       'store_name.MEAN(data.product_width)',\n",
       "       'store_name.MEAN(data.product_length)',\n",
       "       'store_name.MEAN(data.shoe_size)', 'store_name.MEAN(data.price)',\n",
       "       'store_name.COUNT(data)', 'store_name.NUM_UNIQUE(data.condition)',\n",
       "       'store_name.NUM_UNIQUE(data.category)',\n",
       "       'material.MAX(data.image_height)', 'material.MAX(data.price)',\n",
       "       'material.MIN(data.image_width)', 'material.MIN(data.image_height)',\n",
       "       'material.MIN(data.shoe_size)', 'material.MIN(data.price)',\n",
       "       'material.MEAN(data.image_width)', 'material.MEAN(data.price)',\n",
       "       'material.NUM_UNIQUE(data.store_name)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "train_X = train.copy()\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X.select_dtypes([np.number]).fillna(-1), train_y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(train_X.select_dtypes([np.number]).fillna(-1))\n",
    "X_selected_df = pd.DataFrame(X_new, columns=[train_X.select_dtypes([np.number]).fillna(-1).columns[i] for i in range(len(train_X.select_dtypes([np.number]).fillna(-1).columns)) if model.get_support()[i]])\n",
    "print(X_selected_df.shape)\n",
    "X_selected_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11850 entries, 0 to 11849\n",
      "Columns: 181 entries, id to product_description_svd_11\n",
      "dtypes: category(80), float16(82), float32(8), float64(11)\n",
      "memory usage: 5.5 MB\n"
     ]
    }
   ],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_target_and_df(train,label_column):\n",
    "    return train.drop([label_column], axis=1),train[label_column]\n",
    "    \n",
    "def run_randomforest_classifier(train, test, label_column,scoring='accuracy'):\n",
    "    \n",
    "    train,target = split_target_and_df(train,label_column)\n",
    "    \n",
    "    params = {'bootstrap': True, \n",
    "              'class_weight': None, \n",
    "              'criterion': 'gini', \n",
    "              'max_depth': None,\n",
    "              'max_features': 'auto', \n",
    "              'max_leaf_nodes': None, \n",
    "              'min_impurity_decrease': 0.0, \n",
    "              'min_impurity_split': None,\n",
    "              'min_samples_leaf': 1,\n",
    "              'min_samples_split': 2, \n",
    "              'min_weight_fraction_leaf': 0.0, \n",
    "              'n_estimators': 10,\n",
    "              'n_jobs': -1, \n",
    "              'oob_score': False, \n",
    "              'random_state': None, \n",
    "              'verbose': 0, \n",
    "              'warm_start': False}\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(train, target)\n",
    "    pred_train = model.predict(train)\n",
    "    pred_test = model.predict(test)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, train, target, cv=5, scoring=scoring)\n",
    "    print(cv_scores)\n",
    "    print('RF CV mean : %.2f ' % (np.mean(cv_scores)))\n",
    "    print('RF CV std : %.2f ' % (np.std(cv_scores)))\n",
    "        \n",
    "    print(\"True Distribution:\")\n",
    "    print(pd.value_counts(target, normalize=True).sort_index())\n",
    "    print(\"Train Predicted Distribution:\")\n",
    "    print(pd.value_counts(pred_train, normalize=True).sort_index())\n",
    "    print(\"Test Predicted Distribution:\")\n",
    "    print(pd.value_counts(pred_test, normalize=True).sort_index())\n",
    "    \n",
    "    features_importances = pd.Series(model.feature_importances_, index=train.columns)\n",
    "    features_importances.nlargest(25).plot(kind='barh')\n",
    "    \n",
    "    return pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'https://d1kvfoyrif6wzg.cloudfront.net/assets/images/None/main/100_6771_3b0f897.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-5c79a72b956d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_randomforest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-8e905f0b109d>\u001b[0m in \u001b[0;36mrun_randomforest_classifier\u001b[1;34m(train, test, label_column, scoring)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mpred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'https://d1kvfoyrif6wzg.cloudfront.net/assets/images/None/main/100_6771_3b0f897.JPG'"
     ]
    }
   ],
   "source": [
    "pred_test = run_randomforest_classifier(train,test,\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TODO : \"LightGBM validation CV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 2\n",
    "pred_test = run_lgbm(train, test,'label',test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(train, test, label_column):\n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.fit_transform(test)\n",
    "    \n",
    "    svm_params = {'C': 1.0, \n",
    "                  'cache_size': 200, \n",
    "                  'class_weight': None, \n",
    "                  'coef0': 0.0, \n",
    "                  'decision_function_shape': 'ovr', \n",
    "                  'degree': 3, 'gamma': \n",
    "                  'auto_deprecated', \n",
    "                  'kernel': 'rbf', \n",
    "                  'max_iter': -1, \n",
    "                  'probability': False, \n",
    "                  'random_state': None, \n",
    "                  'shrinking': True, \n",
    "                  'tol': 0.001, \n",
    "                  'verbose': False}\n",
    "    \n",
    "    svc=SVC() \n",
    "    svc.fit(train_scaled,target)\n",
    "    y_pred_train=svc.predict(train_scaled)\n",
    "    score = accuracy_score(target,y_pred_train)\n",
    "    print('Accuracy Score: %.2f' % (score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_svm(train, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_voting_classifier(train, test, label_column):\n",
    "    \n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "    \n",
    "    ab_params = {'algorithm': 'SAMME.R', \n",
    "                 'base_estimator': None, \n",
    "                 'learning_rate': 0.1, \n",
    "                 'n_estimators': 20, \n",
    "                 'random_state': None}\n",
    "    \n",
    "    gbc_params = {'criterion': 'friedman_mse', \n",
    "                  'init': None, 'learning_rate': 0.1, \n",
    "                  'loss': 'deviance', \n",
    "                  'max_depth': 30, \n",
    "                  'max_features': None, \n",
    "                  'max_leaf_nodes': None, \n",
    "                  'min_impurity_decrease': 0.0, \n",
    "                  'min_impurity_split': None, \n",
    "                  'min_samples_leaf': 1, \n",
    "                  'min_samples_split': 2, \n",
    "                  'min_weight_fraction_leaf': 0.0, \n",
    "                  'n_estimators': 100, \n",
    "                  'n_iter_no_change': None, \n",
    "                  'presort': 'auto', \n",
    "                  'random_state': None, \n",
    "                  'subsample': 1.0, \n",
    "                  'tol': 0.0001, \n",
    "                  'validation_fraction': 0.1, \n",
    "                  'verbose': 0, \n",
    "                  'warm_start': False}\n",
    "    \n",
    "    bc_params = {'base_estimator': None, \n",
    "                 'bootstrap': True, \n",
    "                 'bootstrap_features': False, \n",
    "                 'max_features': 10, \n",
    "                 'max_samples': 1.0, \n",
    "                 'n_estimators': 20, \n",
    "                 'n_jobs': None, \n",
    "                 'oob_score': False, \n",
    "                 'random_state': None, \n",
    "                 'verbose': 0, \n",
    "                 'warm_start': False}\n",
    "    \n",
    "    clf1 = AdaBoostClassifier(**ab_params)\n",
    "    clf2 = GradientBoostingClassifier(**gbc_params)\n",
    "    clf3 = BaggingClassifier(**bc_params)\n",
    "    vote_clf = VotingClassifier(estimators=[('ab', clf1), ('gbc', clf2), ('bc', clf3)], weights=[0.2,1.7,0.6], voting='soft')\n",
    "    vote_clf = vote_clf.fit(train, target)\n",
    "    \n",
    "    pred_train = vote_clf.predict_proba(train)\n",
    "    pred_cv = cross_val_predict(vote_clf, train, np.ravel(target),\n",
    "                            method='predict_proba', cv=5, n_jobs=-1)\n",
    "    pred_test = vote_clf.predict_proba(test)\n",
    "    \n",
    "    print(\"LogLoss on train sample \", log_loss(y_pred=pred_train, y_true=target))\n",
    "    print(\"LogLoss on train sample (CV): \", log_loss(y_pred=pred_cv, y_true=target))\n",
    "    \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = run_voting_classifier(train, test, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_classifier(train, test, label_column):\n",
    "    \n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "\n",
    "    params = {'objective' : 'multi:softprob', \n",
    "              'num_class'  : 3,\n",
    "              'eval_metric' : 'mlogloss',\n",
    "              'nthread' : -1, \n",
    "              'booster' : \"gbtree\",\n",
    "              'gamma' : 0.1, \n",
    "              'max_depth' : 5,\n",
    "              'eta' : 0.1,\n",
    "              'min_child_weight'  : 0.7\n",
    "             }\n",
    "\n",
    "    clf_xgb = XGBClassifier(**params)\n",
    "\n",
    "    ppl = Pipeline([(\"clf\", clf_xgb)])\n",
    "\n",
    "    ppl.fit(train, np.ravel(y))\n",
    "\n",
    "    pred_train = ppl.predict_proba(train)\n",
    "    pred_cv = cross_val_predict(ppl, train, np.ravel(y),\n",
    "                                method='predict_proba', cv=5, n_jobs=-1,verbose=1)\n",
    "\n",
    "    print(\"LogLoss on train sample:\",log_loss(y_pred=pred_train, y_true=y))\n",
    "    print(\"LogLoss on train sample (CV):\",log_loss(y_pred=pred_cv, y_true=y))\n",
    "    \n",
    "    pred_test = ppl.predict_proba(test)\n",
    "    return pred_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss on train sample: 0.24616178027238814\n",
      "LogLoss on train sample (CV): 1.4492864786074622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06511849, 0.8484266 , 0.08645495],\n",
       "       [0.20762211, 0.47952724, 0.31285062],\n",
       "       [0.06228445, 0.33334592, 0.60436964],\n",
       "       ...,\n",
       "       [0.07604016, 0.13513353, 0.78882635],\n",
       "       [0.05678065, 0.37313035, 0.57008904],\n",
       "       [0.21156481, 0.36582002, 0.42261523]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_xgb_classifier(train._get_numeric_data().fillna(train._get_numeric_data().mean()),test._get_numeric_data().fillna(test._get_numeric_data().mean()),'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-c978ccc675ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mcv_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_log_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RF CV mean : %.2f '\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 573\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "label_column = 'label'\n",
    "train_,target = split_target_and_df(train._get_numeric_data().fillna(train._get_numeric_data().mean()),label_column)\n",
    "\n",
    "model = ExtraTreesClassifier(bootstrap=True , \n",
    "                                         criterion=\"gini\", \n",
    "                                         min_samples_leaf=10, \n",
    "                                         min_samples_split=100, \n",
    "                                         n_estimators=300,\n",
    "                                         random_state = 50,\n",
    "                                         n_jobs = -1)\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(model, train_ , target, cv=5, scoring='neg_log_loss')\n",
    "print(cv_scores)\n",
    "print('RF CV mean : %.2f ' % (np.mean(cv_scores)))\n",
    "print('RF CV std : %.2f ' % (np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(pred_test, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"submission.csv\", index_label=\"id\", header=['0', '1', '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
