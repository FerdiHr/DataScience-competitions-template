{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, Imputer\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "stop_words_fr = get_stop_words('fr')\n",
    "stop_words_en = get_stop_words('en')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce memory usage \n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"X_train.csv\", index_col=0, error_bad_lines=False)\n",
    "test = pd.read_csv(\"X_test.csv\", index_col=0, error_bad_lines=False)\n",
    "y = pd.read_csv(\"y_train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tfidf_vectorizer(df, column):\n",
    "    df[column] = df[column].fillna(\"missing\")\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), stop_words = stop_words_fr, lowercase=True, \n",
    "                                     max_features=50, binary=True, norm=None,use_idf=False)\n",
    "    tfidf = vectorizer.fit_transform(df[column])\n",
    "    tfidf_cols = vectorizer.get_feature_names()\n",
    "    tmp = pd.DataFrame(data=tfidf.toarray(), columns=['tfidf_' + column + '_' + i for i in tfidf_cols])\n",
    "    df = pd.concat([df, tmp], axis=1)\n",
    "    return df\n",
    "\n",
    "def count_item_column(df, column_to_count, column_groupby):\n",
    "    rescuer_count = df.groupby([column_to_count])[column_groupby].count().reset_index()\n",
    "    rescuer_count.rename(columns={rescuer_count.columns[0]: column_to_count}, inplace=True)\n",
    "    rescuer_count.columns = [column_to_count, column_to_count+'_COUNT']\n",
    "    df = df.merge(rescuer_count, how='left', on=column_to_count)\n",
    "    return df\n",
    "\n",
    "def binarie_fill(df,column):\n",
    "    df[column] = df[column].fillna(0)\n",
    "    if True in df[column].tolist():\n",
    "        df[column]= np.where(df[column]==True,1,0)\n",
    "    else:\n",
    "        df[column]= np.where(df[column]==0,0,1)\n",
    "    return df\n",
    "\n",
    "def label_encoding(df,columns_to_encode):\n",
    "    labelencoder = LabelEncoder()\n",
    "    categ_cols = columns_to_encode\n",
    "    for columns_ in categ_cols:\n",
    "        df[columns_+'_ENCODED'] = labelencoder.fit_transform(df[columns_].values.astype(str))\n",
    "    return df\n",
    "\n",
    "def transform_to_log(df,columns_to_log):\n",
    "    for col_ in columns_to_log:\n",
    "        df['log_' + col_] = (1+df[col_]).apply(np.log)\n",
    "        df.drop(col_, inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "def get_len_columns(df, len_columns):\n",
    "    for col_ in len_columns:\n",
    "        df[\"len_\" + col_] = df[col_].str.len()\n",
    "    return df\n",
    "\n",
    "def tfidf_nmf_svd(df,text_columns):\n",
    "    for col_ in tqdm(text_columns):\n",
    "        print(col_)\n",
    "        text = df[col_].values.tolist()\n",
    "        print('[INFO] Start count vectorize')\n",
    "        cvec = CountVectorizer(min_df=2, ngram_range=(1, 3), max_features=1000,\n",
    "                               strip_accents='unicode',\n",
    "                               lowercase=True, analyzer='word', token_pattern=r'\\w+',\n",
    "                               stop_words=stop_words_fr)\n",
    "\n",
    "        cvec.fit(text)\n",
    "        X = cvec.transform(text)\n",
    "        df['cvec_sum'] = X.sum(axis=1)\n",
    "        df['cvec_mean'] = X.mean(axis=1)\n",
    "        df['cvec_len'] = (X != 0).sum(axis=1)\n",
    "\n",
    "        print('[INFO] Start TFDIDF')\n",
    "        tfv = TfidfVectorizer(min_df=2, max_features=1000,\n",
    "                              strip_accents='unicode', analyzer='word',\n",
    "                              ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                              stop_words=stop_words_fr)\n",
    "\n",
    "        # Fit TFIDF\n",
    "        X = tfv.fit_transform(text)\n",
    "        df['tfidf_sum'] = X.sum(axis=1)\n",
    "        df['tfidf_mean'] = X.mean(axis=1)\n",
    "        df['tfidf_len'] = (X != 0).sum(axis=1)\n",
    "        n_components = 20\n",
    "\n",
    "        print('[INFO] Start NMF')\n",
    "\n",
    "        nmf_ = NMF(n_components=n_components)\n",
    "        X_nmf = nmf_.fit_transform(X)\n",
    "        X_nmf = pd.DataFrame(X_nmf, columns=['{}_nmf_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_nmf['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_nmf.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "\n",
    "        print('[INFO] Start SVD')\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        svd.fit(X)\n",
    "        print('fit done')\n",
    "        X_svd = svd.transform(X)\n",
    "        X_svd = pd.DataFrame(X_svd, columns=['{}_svd_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_svd['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_svd.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "        df.drop(col_, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_engineering(df, columns_to_drop, column_to_encode, column_to_vectorize, \n",
    "                         binary_column, text_columns, count_columns, columns_to_log, len_columns):\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.rename(index=str, columns={\"index\": \"id\"})\n",
    "    \n",
    "    \"\"\"\n",
    "    DROP NOT RELEVANT COLUMN \n",
    "    \"\"\"\n",
    "    \n",
    "    df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    \n",
    "    \"\"\"\n",
    "    TEXT FEATURES\n",
    "    \"\"\"\n",
    "    \n",
    "    df = get_len_columns(df, len_columns)\n",
    "\n",
    "    df[text_columns] = df[text_columns].fillna('missing')\n",
    "    \n",
    "    df = label_encoding(df, column_to_encode)\n",
    "    \n",
    "    for col_ in count_columns:\n",
    "        df = count_item_column(df, col_, 'id')\n",
    "\n",
    "    for column_ in column_to_vectorize:\n",
    "        if column_ in df.columns :\n",
    "            df=apply_tfidf_vectorizer(df,column_)\n",
    "            df.drop(column_, inplace=True, axis=1)\n",
    "    \n",
    "    for col_ in binary_column:\n",
    "        df = binarie_fill(df,col_)\n",
    "\n",
    "    \"\"\"\n",
    "    NUMERICAL FEATURES\n",
    "    \"\"\"\n",
    "    \n",
    "    df = transform_to_log(df,columns_to_log)\n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop =  []\n",
    "column_to_encode = []\n",
    "column_to_vectorize = []\n",
    "binary_column = []\n",
    "text_columns = []\n",
    "count_columns = []\n",
    "columns_to_log = []\n",
    "len_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = features_engineering(train, columns_to_drop, column_to_encode, column_to_vectorize, binary_column, \n",
    "                             text_columns, count_columns, columns_to_log, len_columns)\n",
    "\n",
    "test = features_engineering(test, columns_to_drop, column_to_encode, column_to_vectorize, binary_column, \n",
    "                            text_columns, count_columns, columns_to_log, len_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "ppl = Pipeline([(\"imputer\", Imputer(strategy='median')),\n",
    "                (\"clf\", clf_rf)])\n",
    "\n",
    "ppl.fit(train, np.ravel(y))\n",
    "\n",
    "pred_train = ppl.predict_proba(train)\n",
    "pred_cv = cross_val_predict(ppl, train, np.ravel(y),\n",
    "                            method='predict_proba', cv=5, n_jobs=-1)\n",
    "\n",
    "print(\"LogLoss on train sample:\",log_loss(y_pred=pred_train, y_true=y))\n",
    "print(\"LogLoss on train sample (CV):\",log_loss(y_pred=pred_cv, y_true=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances = pd.Series(clf_rf.feature_importances_, index=train.columns)\n",
    "features_importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = ppl.predict_proba(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective' : 'multi:softprob', \n",
    "          'num_class'  : 3,\n",
    "          'eval_metric' : 'mlogloss',\n",
    "          'nthread' : -1, \n",
    "          'booster' : \"gbtree\",\n",
    "          'gamma' : 0.01, \n",
    "          'max_depth' : 7,\n",
    "          'eta' : 0.1,\n",
    "          'min_child_weight'  : 0.7\n",
    "         }\n",
    "\n",
    "clf_xgb = XGBClassifier(**params)\n",
    "\n",
    "ppl = Pipeline([(\"clf\", clf_xgb)])\n",
    "\n",
    "ppl.fit(train, np.ravel(y))\n",
    "\n",
    "pred_train = ppl.predict_proba(train)\n",
    "pred_cv = cross_val_predict(ppl, train, np.ravel(y),\n",
    "                            method='predict_proba', cv=5, n_jobs=-1)\n",
    "\n",
    "print(\"LogLoss on train sample:\",log_loss(y_pred=pred_train, y_true=y))\n",
    "print(\"LogLoss on train sample (CV):\",log_loss(y_pred=pred_cv, y_true=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(csr_matrix(train), label=y, feature_names=train.columns.values)\n",
    "d_test = xgb.DMatrix(csr_matrix(test))\n",
    "clf = clf_xgb.fit(train, np.ravel(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "plot_importance(clf, max_num_features=50, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = ppl.predict_proba(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'metric' : 'multi_logloss',\n",
    "    'objective':'multiclass',\n",
    "    'boosting': 'gbdt', \n",
    "    'num_class' : 3,\n",
    "    'subsample': 1, \n",
    "    'colsample_bytree': 0.9, \n",
    "    'min_split_gain': 0.4, \n",
    "    'min_child_weight': 1, \n",
    "    'min_child_samples': 5,\n",
    "    'max_bin': 300, \n",
    "    'num_iterations': 90,\n",
    "    'learning_rate': 0.15,\n",
    "    'subsample_for_bin': 200, \n",
    "    'lambda_l1': 0, \n",
    "    'lambda_l2': 0, \n",
    "    'num_leaves': 80,\n",
    "    'max_depth': 25, \n",
    "    'reg_alpha' : 1.2,\n",
    "    'reg_lambda' : 1.2,\n",
    "}\n",
    "\n",
    "clf_lgb = lgb.LGBMClassifier(**params)\n",
    "\n",
    "ppl = Pipeline([(\"imputer\", Imputer(strategy='median')),\n",
    "                (\"clf\", clf_lgb)])\n",
    "\n",
    "ppl.fit(train, np.ravel(y))\n",
    "\n",
    "pred_train = ppl.predict_proba(train)\n",
    "pred_cv = cross_val_predict(ppl, train, np.ravel(y),\n",
    "                            method='predict_proba', cv=5, n_jobs=-1)\n",
    "\n",
    "print(\"LogLoss on train sample:\",log_loss(y_pred=pred_train, y_true=y))\n",
    "print(\"LogLoss on train sample (CV):\",log_loss(y_pred=pred_cv, y_true=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = ppl.predict_proba(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(pred_test, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"submission.csv\", index_label=\"id\", header=['0', '1', '2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
