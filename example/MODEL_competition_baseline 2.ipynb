{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Notebook - DataScience Competition Baseline\n",
    "\n",
    "### Created by Anis Ayari : https://github.com/anisayari on May 2019\n",
    "\n",
    "Please consider to report any enhancements/bug/modification/use to : aayari@deloitte.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datascience-env/lib/python3.6/site-packages/matplotlib/__init__.py:1003: UserWarning: Duplicate key in file \"/Users/anisayari/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/anaconda3/envs/datascience-env/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#DS & Math\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#Vizu libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.decomposition import TruncatedSVD,NMF\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, OneHotEncoder\n",
    "from sklearn.model_selection import KFold,cross_val_score,cross_val_predict, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Other ML libraries\n",
    "import featuretools as ft\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from stop_words import get_stop_words\n",
    "stop_words_fr = get_stop_words('fr')\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "#Others\n",
    "import warnings\n",
    "import csv \n",
    "import os \n",
    "import time \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FEATURE ENGINEERING COMMON FUNCTIONS\n",
    "\"\"\"\n",
    "#@TODO : 'Need to check with auto FE libraries\n",
    "\"\"\"\n",
    "MATHEMATICS FEATURES\n",
    "\"\"\"\n",
    "def create_mathematics_features(df, column_to_count, column_to_groupby):\n",
    "    df_tmp = df.groupby(column_to_groupby)[column_to_count].agg(['count','mean', 'std', 'max', 'min'])\n",
    "    df_tmp.columns =['count_' + column_to_count, 'mean_' + column_to_count, 'std_' + column_to_count,'max_' + column_to_count, 'min_' +column_to_count,]\n",
    "    df = df.merge(df_tmp, on=column_to_groupby, how='left')\n",
    "    return df \n",
    "\n",
    "\"\"\"\n",
    "NUMERICAL FEATURES\n",
    "\"\"\"\n",
    "def get_len_columns(df, len_columns):\n",
    "    for col_ in len_columns:\n",
    "        df[\"len_\" + col_] = df[col_].str.len()\n",
    "    return df\n",
    "\n",
    "def transform_to_log(df,columns_to_log):\n",
    "    for col_ in columns_to_log:\n",
    "        df['log_' + col_] = (1+df[col_]).apply(np.log)\n",
    "    return df\n",
    "\n",
    "def count_product_per_store(df, column_to_groupby, column_to_count):\n",
    "    tmp = df.groupby(column_to_groupby).count()[column_to_count].reset_index()\n",
    "    tmp.columns = [column_to_groupby] + [\"number_\" + column_to_count + '_' + column_to_groupby]\n",
    "    df = df.merge(tmp, on=column_to_groupby, how='left')\n",
    "    return df\n",
    "\n",
    "def count_item_column(df, column_to_count, column_groupby):\n",
    "    rescuer_count = df.groupby([column_to_count])[column_groupby].count().reset_index()\n",
    "    rescuer_count.rename(columns={rescuer_count.columns[0]: column_to_count}, inplace=True)\n",
    "    rescuer_count.columns = [column_to_count, column_to_count+'_COUNT']\n",
    "    df = df.merge(rescuer_count, how='left', on=column_to_count)\n",
    "    return df\n",
    "\n",
    "def label_encoding(df,columns_to_encode):\n",
    "    labelencoder = LabelEncoder()\n",
    "    categ_cols = columns_to_encode\n",
    "    for columns_ in categ_cols:\n",
    "        df[columns_+'_ENCODED'] = labelencoder.fit_transform(df[columns_].values.astype(str))\n",
    "    return df\n",
    "\n",
    "def binarie_fill(df,column):\n",
    "    df[column] = df[column].fillna(0)\n",
    "    if True in df[column].tolist():\n",
    "        df[column]= np.where(df[column]==True,1,0)\n",
    "    else:\n",
    "        df[column]= np.where(df[column]==0,0,1)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "TEXT\n",
    "\"\"\"\n",
    "\n",
    "def apply_tfidf_vectorizer(df, column):\n",
    "    df[column] = df[column].fillna(\"missing\")\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), stop_words = stop_words_fr, lowercase=True, \n",
    "                                     max_features=50, binary=True, norm=None,use_idf=False)\n",
    "    tfidf = vectorizer.fit_transform(df[column])\n",
    "    tfidf_cols = vectorizer.get_feature_names()\n",
    "    tmp = pd.DataFrame(data=tfidf.toarray(), columns=['tfidf_' + column + '_' + i for i in tfidf_cols])\n",
    "    df = pd.concat([df, tmp], axis=1)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "IMAGE\n",
    "\"\"\"\n",
    "#@TODO : 'To fill'\n",
    "\n",
    "\"\"\"\n",
    "SONG\n",
    "\"\"\"\n",
    "#@TODO : 'To fill'\n",
    "\n",
    "\n",
    "def tfidf_nmf_svd(df,text_columns):\n",
    "    for col_ in tqdm(text_columns):\n",
    "        print(col_)\n",
    "        text = df[col_].values.tolist()\n",
    "        print('[INFO] Start count vectorize')\n",
    "        cvec = CountVectorizer(min_df=2, ngram_range=(1, 3), max_features=1000,\n",
    "                               strip_accents='unicode',\n",
    "                               lowercase=True, analyzer='word', token_pattern=r'\\w+',\n",
    "                               stop_words=stop_words_fr)\n",
    "\n",
    "        cvec.fit(text)\n",
    "        X = cvec.transform(text)\n",
    "        df['cvec_sum'] = X.sum(axis=1)\n",
    "        df['cvec_mean'] = X.mean(axis=1)\n",
    "        df['cvec_len'] = (X != 0).sum(axis=1)\n",
    "\n",
    "        print('[INFO] Start TFDIDF')\n",
    "        tfv = TfidfVectorizer(min_df=2, max_features=1000,\n",
    "                              strip_accents='unicode', analyzer='word',\n",
    "                              ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                              stop_words=stop_words_fr)\n",
    "\n",
    "        # Fit TFIDF\n",
    "        X = tfv.fit_transform(text)\n",
    "        df['tfidf_sum'] = X.sum(axis=1)\n",
    "        df['tfidf_mean'] = X.mean(axis=1)\n",
    "        df['tfidf_len'] = (X != 0).sum(axis=1)\n",
    "        n_components = 20\n",
    "\n",
    "        print('[INFO] Start NMF')\n",
    "\n",
    "        nmf_ = NMF(n_components=n_components)\n",
    "        X_nmf = nmf_.fit_transform(X)\n",
    "        X_nmf = pd.DataFrame(X_nmf, columns=['{}_nmf_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_nmf['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_nmf.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "\n",
    "        print('[INFO] Start SVD')\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        svd.fit(X)\n",
    "        print('fit done')\n",
    "        X_svd = svd.transform(X)\n",
    "        X_svd = pd.DataFrame(X_svd, columns=['{}_svd_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_svd['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_svd.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "        df.drop(col_, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def auto_features(df):\n",
    "    print('[INFO] Auto Features Processing')\n",
    "    \n",
    "    es = ft.EntitySet(id = 'emmaus')\n",
    "    #es = es.entity_from_dataframe(entity_id = 'data',dataframe = train_test.reset_index(drop=True),make_index = True,index='id')\n",
    "    es = es.entity_from_dataframe(entity_id='data', index='id', dataframe = df)\n",
    "\n",
    "    for groupby in ['brand','category','store_name','product_name','material']:\n",
    "        es = es.normalize_entity(base_entity_id='data', new_entity_id=groupby, index=groupby)\n",
    "    \n",
    "    features, feature_names = ft.dfs(entityset = es, target_entity = 'data', max_depth = 2, verbose=2, n_jobs=5)\n",
    "\n",
    "    # Threshold for removing correlated variables\n",
    "    threshold = 0.95\n",
    "\n",
    "    # Absolute value correlation matrix\n",
    "    corr_matrix = features.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    upper.head(50)\n",
    "\n",
    "    # Select columns with correlations above threshold\n",
    "    collinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    print('There are %d features to remove.' % (len(collinear_features)))\n",
    "\n",
    "    features_filtered = features.drop(columns = collinear_features)\n",
    "\n",
    "    print('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\n",
    "    features_positive = features_filtered.loc[:, features_filtered.all()]\n",
    "\n",
    "    pd.concat([df.set_index('id'), features_positive].set_index('id'), axis=1, sort=False)\n",
    "    print('[INFO] Auto Features DONE')\n",
    "    return df\n",
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    \"\"\"\n",
    "    DROP NOT RELEVANT COLUMN \n",
    "    \"\"\"\n",
    "    print('[INFO] Dropping Columns...')\n",
    "    columns_to_drop = [\"image_url\", \"sub_category_3\", \"sub_category_4\"]  #'To fill'\n",
    "    df.drop(columns_to_drop, axis = 1, inplace = True)    \n",
    "    \"\"\"\n",
    "    TEXT FEATURES\n",
    "    \"\"\"\n",
    "    print('[INFO] Text Features processing')\n",
    "    column_to_count = 'price'    #'To fill'\n",
    "    column_to_groupby = 'store_name'    #'To fill'\n",
    "    df = create_mathematics_features(df, column_to_count, column_to_groupby)\n",
    "    \n",
    "    len_columns = ['product_description']  #'To fill'\n",
    "    df = get_len_columns(df, len_columns)\n",
    "\n",
    "    text_columns = ['product_description', 'product_name', 'material']  #'To fill'\n",
    "    df[text_columns] = df[text_columns].fillna('missing')\n",
    "    \n",
    "    column_to_encode = ['color','age','product_size',\"brand\",\"shoe_size\"]  #'To fill'\n",
    "    df = label_encoding(df, column_to_encode)\n",
    "        \n",
    "    count_column = [\"brand\", \"author\", \"editor\"]  #'To fill'\n",
    "    for col_ in count_column:\n",
    "        df = count_item_column(df, col_, 'id')\n",
    "    \n",
    "    column_to_vectorize = [\"sub_category_1\", \"sub_category_2\",'store_name','product_description',\n",
    "                    'material', 'editor', 'product_name',\"author\"]  #'To fill'\n",
    "    for column_ in column_to_vectorize:\n",
    "        if column_ in df.columns :\n",
    "            df=apply_tfidf_vectorizer(df,column_)\n",
    "            df.drop(column_, inplace=True, axis=1)\n",
    "    \n",
    "    binary_column = ['warranty','wifi','vintage']  #'To fill'\n",
    "    for col_ in binary_column:\n",
    "        df = binarie_fill(df,col_)\n",
    "    \n",
    "    columns_to_dummies = ['category']  # 'To fill'\n",
    "    for col_ in columns_to_dummies:\n",
    "        df = pd.concat([df.drop(col_, axis=1), pd.get_dummies(df[col_],prefix=col_)], axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    NUMERICAL FEATURES\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_to_log = [\"price\", \"len_product_description\"]  #'To fill'\n",
    "    transform_to_log(df,columns_to_log)\n",
    "\n",
    "    to_drop = [\"price\",\"id\",'image_width','image_height','color','age',\n",
    "             'product_size',\"brand\",\"shoe_size\",\"len_product_description\", \n",
    "               \"condition\", \"year\", \"product_width\",\"product_length\", \"product_height\"]  #'To fill'\n",
    "    df.drop(to_drop,inplace=True, axis=1)\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2168: expected 31 fields, saw 33\\nSkipping line 4822: expected 31 fields, saw 37\\nSkipping line 4859: expected 31 fields, saw 37\\nSkipping line 7342: expected 31 fields, saw 37\\n'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mradd\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mradd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mmasked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mradd\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mradd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-a652fb16c360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   2028\u001b[0m             return _combine_series_frame(self, other, pass_op,\n\u001b[1;32m   2029\u001b[0m                                          \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m                                          level=level)\n\u001b[0m\u001b[1;32m   2031\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_combine_series_frame\u001b[0;34m(self, other, func, fill_value, axis, level)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m         \u001b[0;31m# default axis is columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1930\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_match_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_combine_match_columns\u001b[0;34m(self, other, func, level)\u001b[0m\n\u001b[1;32m   5114\u001b[0m                                  copy=False)\n\u001b[1;32m   5115\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_to_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mdispatch_to_series\u001b[0;34m(left, right, func, str_rep, axis)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m     \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0muse_numexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mcolumn_op\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcolumn_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             return {i: func(a.iloc[:, i], b.iloc[i])\n\u001b[0;32m-> 1144\u001b[0;31m                     for i in range(len(a.columns))}\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcolumn_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             return {i: func(a.iloc[:, i], b.iloc[i])\n\u001b[0;32m-> 1144\u001b[0;31m                     for i in range(len(a.columns))}\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mradd\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mradd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m         return construct_result(left, result,\n\u001b[1;32m   1585\u001b[0m                                 index=left.index, name=res_name, dtype=None)\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m                 return libalgos.arrmap_object(lvalues,\n\u001b[0;32m-> 1533\u001b[0;31m                                               lambda x: op(x, rvalues))\n\u001b[0m\u001b[1;32m   1534\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/algos.pyx\u001b[0m in \u001b[0;36mpandas._libs.algos.arrmap\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m                 return libalgos.arrmap_object(lvalues,\n\u001b[0;32m-> 1533\u001b[0;31m                                               lambda x: op(x, rvalues))\n\u001b[0m\u001b[1;32m   1534\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mradd\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mradd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"X_train.csv\", index_col=0, error_bad_lines=False)\n",
    "len_train = len(train)\n",
    "test = pd.read_csv(\"X_test.csv\", index_col=0, error_bad_lines=False)\n",
    "\n",
    "train = train.reset_index()\n",
    "test= test.reset_index()\n",
    "#t\n",
    "rain['id'] = train['id'].astype(str)+'_'+train\n",
    "y = pd.read_csv(\"y_train.csv\", index_col=0)\n",
    "train_test = pd.concat((train, test), axis=0)\n",
    "train_test=train_test.reset_index()\n",
    "#train_test = features_engineering(train_test)\n",
    "#train_test = train_test.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          1\n",
       "2          2\n",
       "3          3\n",
       "4          4\n",
       "5          5\n",
       "6          6\n",
       "7          7\n",
       "8          8\n",
       "9          9\n",
       "10        10\n",
       "11        11\n",
       "12        12\n",
       "13        13\n",
       "14        14\n",
       "15        15\n",
       "16        16\n",
       "17        17\n",
       "18        18\n",
       "19        19\n",
       "20        20\n",
       "21        21\n",
       "22        22\n",
       "23        23\n",
       "24        24\n",
       "25        25\n",
       "26        26\n",
       "27        27\n",
       "28        28\n",
       "29        29\n",
       "        ... \n",
       "8850    8850\n",
       "8851    8851\n",
       "8852    8852\n",
       "8853    8853\n",
       "8854    8854\n",
       "8855    8855\n",
       "8856    8856\n",
       "8857    8857\n",
       "8858    8858\n",
       "8859    8859\n",
       "8860    8860\n",
       "8861    8861\n",
       "8862    8862\n",
       "8863    8863\n",
       "8864    8864\n",
       "8865    8865\n",
       "8866    8866\n",
       "8867    8867\n",
       "8868    8868\n",
       "8869    8869\n",
       "8870    8870\n",
       "8871    8871\n",
       "8872    8872\n",
       "8873    8873\n",
       "8874    8874\n",
       "8875    8875\n",
       "8876    8876\n",
       "8877    8877\n",
       "8878    8878\n",
       "8879    8879\n",
       "Name: id, Length: 8880, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 7 memory: 1471 MB fds: 160>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/ioloop.py\", line 907, in _run\n",
      "    return self.callback()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/system_monitor.py\", line 67, in update\n",
      "    read_bytes = (ioc.bytes_recv - last.bytes_recv) / (duration or 0.5)\n",
      "AttributeError: 'NoneType' object has no attribute 'bytes_recv'\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Nanny for 29.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
     ]
    }
   ],
   "source": [
    "train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Index is not unique on dataframe (Entity data)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-f752810eb7f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/featuretools/entityset/entityset.py\u001b[0m in \u001b[0;36mentity_from_dataframe\u001b[0;34m(self, entity_id, dataframe, index, variable_types, make_index, time_index, secondary_time_index, already_sorted)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0msecondary_time_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_time_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0malready_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malready_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             make_index=make_index)\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_data_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/featuretools/entityset/entity.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, id, df, entityset, variable_types, index, time_index, secondary_time_index, last_time_index, already_sorted, make_index, verbose)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/featuretools/entityset/entity.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, variable_id, unique)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Index is not unique on dataframe (Entity {})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variable_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Index is not unique on dataframe (Entity data)"
     ]
    }
   ],
   "source": [
    "es.entity_from_dataframe(entity_id='data', index='id', dataframe = train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Auto Features Processing\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Index is not unique on dataframe (Entity data)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-65711348f6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauto_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-81ab8ccf29de>\u001b[0m in \u001b[0;36mauto_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEntitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'emmaus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m#es = es.entity_from_dataframe(entity_id = 'data',dataframe = train_test.reset_index(drop=True),make_index = True,index='id')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgroupby\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'brand'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'store_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'product_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'material'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/featuretools/entityset/entityset.py\u001b[0m in \u001b[0;36mentity_from_dataframe\u001b[0;34m(self, entity_id, dataframe, index, variable_types, make_index, time_index, secondary_time_index, already_sorted)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0msecondary_time_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_time_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0malready_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malready_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             make_index=make_index)\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_data_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/featuretools/entityset/entity.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, id, df, entityset, variable_types, index, time_index, secondary_time_index, last_time_index, already_sorted, make_index, verbose)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/featuretools/entityset/entity.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, variable_id, unique)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Index is not unique on dataframe (Entity {})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variable_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Index is not unique on dataframe (Entity data)"
     ]
    }
   ],
   "source": [
    "auto_features(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_count',\n",
       " 'image_width',\n",
       " 'image_height',\n",
       " 'image_url',\n",
       " 'product_description',\n",
       " 'product_size',\n",
       " 'material',\n",
       " 'age',\n",
       " 'warranty',\n",
       " 'year',\n",
       " 'color',\n",
       " 'product_width',\n",
       " 'wifi',\n",
       " 'condition',\n",
       " 'product_length',\n",
       " 'shoe_size',\n",
       " 'vintage',\n",
       " 'brand',\n",
       " 'author',\n",
       " 'editor',\n",
       " 'product_height',\n",
       " 'weight',\n",
       " 'price',\n",
       " 'category',\n",
       " 'sub_category_1',\n",
       " 'sub_category_2',\n",
       " 'sub_category_3',\n",
       " 'sub_category_4',\n",
       " 'product_name',\n",
       " 'store_name']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPercetage of NaN values in Train\\npd.DataFrame({'number_of_nan_train':train.isna().sum().tolist(),\\n              'percentage_of_nan_train': (train.isna().mean()* 100).round(1).tolist(),\\n             'number_of_nan_test':test.isna().sum().tolist(),\\n              'percentage_of_nan_test': (test.isna().mean()* 100).round(1).tolist()},\\n             index=train.columns).sort_values(by=['number_of_nan_train'], ascending=False)\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Percetage of NaN values in Train\n",
    "pd.DataFrame({'number_of_nan_train':train.isna().sum().tolist(),\n",
    "              'percentage_of_nan_train': (train.isna().mean()* 100).round(1).tolist(),\n",
    "             'number_of_nan_test':test.isna().sum().tolist(),\n",
    "              'percentage_of_nan_test': (test.isna().mean()* 100).round(1).tolist()},\n",
    "             index=train.columns).sort_values(by=['number_of_nan_train'], ascending=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Auto Features Processing\n",
      "Built 1558 features\n",
      "EntitySet scattered to 4 workers in 3 seconds\n",
      "Elapsed: 00:05 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dropping Columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Client.__del__ of <Client: not connected>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1075, in __del__\n",
      "    self.close()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1290, in close\n",
      "    sync(self.loop, self._close, fast=True)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 331, in sync\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['image_url'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1cb0cd877860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#train_test = train_test.dropna(axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3a178ecc8905>\u001b[0m in \u001b[0;36mfeatures_engineering\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Dropping Columns...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mcolumns_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"image_url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sub_category_3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sub_category_4\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#'To fill'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4963\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4964\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4965\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['image_url'] not found in axis\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    six.reraise(*error[0])\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/six.py\", line 693, in reraise\n",
      "    raise value\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 316, in f\n",
      "    result[0] = yield future\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError: \n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 992, in _reconnect\n",
      "    yield self._close()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entityset: emmaus\n",
      "  Entities:\n",
      "    data [Rows: 11840, Columns: 31]\n",
      "    brand [Rows: 3330, Columns: 1]\n",
      "    category [Rows: 12, Columns: 1]\n",
      "    color [Rows: 19, Columns: 1]\n",
      "  Relationships:\n",
      "    data.brand -> brand.brand\n",
      "    data.category -> category.category\n",
      "    data.color -> color.color\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entityset: emmaus\n",
      "  Entities:\n",
      "    data [Rows: 11840, Columns: 31]\n",
      "    brand [Rows: 3330, Columns: 1]\n",
      "    category [Rows: 12, Columns: 1]\n",
      "    color [Rows: 19, Columns: 1]\n",
      "  Relationships:\n",
      "    data.brand -> brand.brand\n",
      "    data.category -> category.category\n",
      "    data.color -> color.color\n",
      "Built 317 features\n",
      "EntitySet scattered to 5 workers in 4 seconds\n",
      "Elapsed: 00:00 | Remaining: ? | Progress:   0%|          | Calculated: 0/10 chunks"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n",
      "Exception ignored in: <bound method Client.__del__ of <Client: not connected>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1075, in __del__\n",
      "    self.close()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1290, in close\n",
      "    sync(self.loop, self._close, fast=True)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 331, in sync\n",
      "    six.reraise(*error[0])\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/six.py\", line 693, in reraise\n",
      "    raise value\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 316, in f\n",
      "    result[0] = yield future\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 00:12 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "There are 101 features to remove.\n",
      "The number of features that passed the collinearity threshold:  216\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearSVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5e1da6f393f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_positive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mlsvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearSVC' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 992, in _reconnect\n",
      "    yield self._close()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8880, 55)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X.select_dtypes([np.number]).fillna(-1), train_y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(train_X.select_dtypes([np.number]).fillna(-1))\n",
    "X_selected_df = pd.DataFrame(X_new, columns=[train_X.select_dtypes([np.number]).fillna(-1).columns[i] for i in range(len(train_X.select_dtypes([np.number]).fillna(-1).columns)) if model.get_support()[i]])\n",
    "X_selected_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_width', 'image_height', 'product_width', 'product_length',\n",
       "       'shoe_size', 'product_height', 'price',\n",
       "       'NUM_WORDS(product_description)', 'brand.MAX(data.image_width)',\n",
       "       'brand.MAX(data.image_height)', 'brand.MAX(data.year)',\n",
       "       'brand.MAX(data.shoe_size)', 'brand.MAX(data.price)',\n",
       "       'brand.MIN(data.shoe_size)', 'brand.MIN(data.price)',\n",
       "       'brand.MEAN(data.image_width)', 'brand.MEAN(data.image_height)',\n",
       "       'brand.MEAN(data.year)', 'brand.COUNT(data)',\n",
       "       'brand.NUM_UNIQUE(data.store_name)', 'category.SUM(data.images_count)',\n",
       "       'category.STD(data.image_width)', 'category.STD(data.image_height)',\n",
       "       'category.MAX(data.images_count)', 'category.MAX(data.image_width)',\n",
       "       'category.MAX(data.image_height)', 'category.MAX(data.product_width)',\n",
       "       'category.MAX(data.shoe_size)', 'category.MAX(data.price)',\n",
       "       'category.NUM_UNIQUE(data.sub_category_1)',\n",
       "       'color.SUM(data.images_count)', 'color.STD(data.image_width)',\n",
       "       'color.STD(data.image_height)', 'color.STD(data.year)',\n",
       "       'color.STD(data.product_width)', 'color.STD(data.product_length)',\n",
       "       'color.MAX(data.image_width)', 'color.MAX(data.image_height)',\n",
       "       'color.MAX(data.year)', 'color.MAX(data.product_width)',\n",
       "       'color.MAX(data.product_length)', 'color.MAX(data.product_height)',\n",
       "       'color.MAX(data.price)', 'color.MIN(data.image_width)',\n",
       "       'color.MIN(data.image_height)', 'color.MIN(data.product_width)',\n",
       "       'color.MIN(data.shoe_size)', 'color.MIN(data.weight)',\n",
       "       'color.MEAN(data.image_width)', 'color.MEAN(data.image_height)',\n",
       "       'color.MEAN(data.year)', 'color.MEAN(data.product_length)',\n",
       "       'color.MEAN(data.product_height)',\n",
       "       'color.NUM_UNIQUE(data.product_size)',\n",
       "       'color.NUM_UNIQUE(data.sub_category_1)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images_count</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>product_size</th>\n",
       "      <th>material</th>\n",
       "      <th>age</th>\n",
       "      <th>warranty</th>\n",
       "      <th>year</th>\n",
       "      <th>color</th>\n",
       "      <th>product_width</th>\n",
       "      <th>...</th>\n",
       "      <th>color.MODE(data.brand)</th>\n",
       "      <th>color.MODE(data.author)</th>\n",
       "      <th>color.MODE(data.editor)</th>\n",
       "      <th>color.MODE(data.category)</th>\n",
       "      <th>color.MODE(data.sub_category_1)</th>\n",
       "      <th>color.MODE(data.sub_category_2)</th>\n",
       "      <th>color.MODE(data.sub_category_3)</th>\n",
       "      <th>color.MODE(data.sub_category_4)</th>\n",
       "      <th>color.MODE(data.product_name)</th>\n",
       "      <th>color.MODE(data.store_name)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3458.0</td>\n",
       "      <td>2552.0</td>\n",
       "      <td>44</td>\n",
       "      <td>100 % polyester</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multicolore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Desigual</td>\n",
       "      <td>GOSCINNY et UDERZO</td>\n",
       "      <td>Dargaud</td>\n",
       "      <td>mode</td>\n",
       "      <td>accessoires femme</td>\n",
       "      <td>label selection</td>\n",
       "      <td>les coups de coeur des vendeurs</td>\n",
       "      <td>Cérémonies</td>\n",
       "      <td>Collection Papillons</td>\n",
       "      <td>Emmaüs 88 Neufchateau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2486.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plastique</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jaune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Ricard</td>\n",
       "      <td>FRANQUIN</td>\n",
       "      <td>G. M. Perrin</td>\n",
       "      <td>mobilier - deco</td>\n",
       "      <td>bibelots et objets déco</td>\n",
       "      <td>label selection</td>\n",
       "      <td>Déco Pop et Colorée</td>\n",
       "      <td>Vélorution !</td>\n",
       "      <td>Bougeoir en laiton</td>\n",
       "      <td>Emmaüs 88 Neufchateau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>40</td>\n",
       "      <td>Polyester, coton, laine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ESPRIT</td>\n",
       "      <td>Dom Norbert Nieuwland et Maurice Tschoffen</td>\n",
       "      <td>Duculot</td>\n",
       "      <td>mode</td>\n",
       "      <td>mode</td>\n",
       "      <td>label selection</td>\n",
       "      <td>les coups de coeur des vendeurs</td>\n",
       "      <td>Mode : Carnaval !</td>\n",
       "      <td>Apple iPhone 5S - 16 Go - Gris sidéral - Très ...</td>\n",
       "      <td>Label Emmaüs Chambéry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6 mois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Blanc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>tmp</td>\n",
       "      <td>BOUTET DE MONVEL</td>\n",
       "      <td>\"avx despens de l'avteur\"</td>\n",
       "      <td>mobilier - deco</td>\n",
       "      <td>mode</td>\n",
       "      <td>label selection</td>\n",
       "      <td>poupées</td>\n",
       "      <td>Cabinet de curiosités</td>\n",
       "      <td>Napperon rectangulaire brodé main en coton</td>\n",
       "      <td>Label Emmaüs Chambéry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       images_count  image_width  image_height product_size  \\\n",
       "index                                                         \n",
       "0                 3       3458.0        2552.0           44   \n",
       "1                 2       2486.0        2254.0          NaN   \n",
       "2                 3       1536.0        1536.0           40   \n",
       "3                 2       1100.0        1100.0          NaN   \n",
       "4                 2        450.0         450.0          NaN   \n",
       "\n",
       "                      material  age warranty  year        color  \\\n",
       "index                                                             \n",
       "0             100 % polyester   NaN      NaN   NaN  Multicolore   \n",
       "1                    Plastique  NaN      NaN   NaN        Jaune   \n",
       "2      Polyester, coton, laine  NaN      NaN   NaN         Gris   \n",
       "3                          NaN  NaN      NaN   NaN          NaN   \n",
       "4                          NaN  NaN   6 mois   NaN        Blanc   \n",
       "\n",
       "       product_width  ... color.MODE(data.brand)  \\\n",
       "index                 ...                          \n",
       "0                NaN  ...               Desigual   \n",
       "1                NaN  ...                 Ricard   \n",
       "2                NaN  ...                 ESPRIT   \n",
       "3                NaN  ...                    NaN   \n",
       "4                NaN  ...                    tmp   \n",
       "\n",
       "                          color.MODE(data.author)    color.MODE(data.editor)  \\\n",
       "index                                                                          \n",
       "0                              GOSCINNY et UDERZO                    Dargaud   \n",
       "1                                        FRANQUIN               G. M. Perrin   \n",
       "2      Dom Norbert Nieuwland et Maurice Tschoffen                    Duculot   \n",
       "3                                             NaN                        NaN   \n",
       "4                               BOUTET DE MONVEL   \"avx despens de l'avteur\"   \n",
       "\n",
       "       color.MODE(data.category) color.MODE(data.sub_category_1)  \\\n",
       "index                                                              \n",
       "0                           mode               accessoires femme   \n",
       "1                mobilier - deco         bibelots et objets déco   \n",
       "2                           mode                            mode   \n",
       "3                            NaN                             NaN   \n",
       "4                mobilier - deco                            mode   \n",
       "\n",
       "      color.MODE(data.sub_category_2)  color.MODE(data.sub_category_3)  \\\n",
       "index                                                                    \n",
       "0                     label selection  les coups de coeur des vendeurs   \n",
       "1                     label selection              Déco Pop et Colorée   \n",
       "2                     label selection  les coups de coeur des vendeurs   \n",
       "3                                 NaN                              NaN   \n",
       "4                     label selection                          poupées   \n",
       "\n",
       "      color.MODE(data.sub_category_4)  \\\n",
       "index                                   \n",
       "0                          Cérémonies   \n",
       "1                        Vélorution !   \n",
       "2                   Mode : Carnaval !   \n",
       "3                                 NaN   \n",
       "4               Cabinet de curiosités   \n",
       "\n",
       "                           color.MODE(data.product_name)  \\\n",
       "index                                                      \n",
       "0                                   Collection Papillons   \n",
       "1                                     Bougeoir en laiton   \n",
       "2      Apple iPhone 5S - 16 Go - Gris sidéral - Très ...   \n",
       "3                                                    NaN   \n",
       "4             Napperon rectangulaire brodé main en coton   \n",
       "\n",
       "       color.MODE(data.store_name)  \n",
       "index                               \n",
       "0            Emmaüs 88 Neufchateau  \n",
       "1            Emmaüs 88 Neufchateau  \n",
       "2            Label Emmaüs Chambéry  \n",
       "3                              NaN  \n",
       "4            Label Emmaüs Chambéry  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11840 entries, 0 to 11839\n",
      "Columns: 431 entries, images_count to log_len_product_description\n",
      "dtypes: float16(420), float32(1), int16(2), int8(8)\n",
      "memory usage: 9.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_test.iloc[:len_train, :]\n",
    "test = train_test.iloc[len_train:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test.index\n",
    "train['label'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_target_and_df(train,label_column):\n",
    "    return train.drop([label_column], axis=1),train[label_column]\n",
    "    \n",
    "def run_randomforest_classifier(train, test, label_column,scoring='accuracy'):\n",
    "    \n",
    "    train,target = split_target_and_df(train,label_column)\n",
    "    \n",
    "    params = {'bootstrap': True, \n",
    "              'class_weight': None, \n",
    "              'criterion': 'gini', \n",
    "              'max_depth': None,\n",
    "              'max_features': 'auto', \n",
    "              'max_leaf_nodes': None, \n",
    "              'min_impurity_decrease': 0.0, \n",
    "              'min_impurity_split': None,\n",
    "              'min_samples_leaf': 1,\n",
    "              'min_samples_split': 2, \n",
    "              'min_weight_fraction_leaf': 0.0, \n",
    "              'n_estimators': 10,\n",
    "              'n_jobs': -1, \n",
    "              'oob_score': False, \n",
    "              'random_state': None, \n",
    "              'verbose': 0, \n",
    "              'warm_start': False}\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(train, target)\n",
    "    pred_train = model.predict(train)\n",
    "    pred_test = model.predict(test)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, train, target, cv=5, scoring=scoring)\n",
    "    print(cv_scores)\n",
    "    print('RF CV mean : %.2f ' % (np.mean(cv_scores)))\n",
    "    print('RF CV std : %.2f ' % (np.std(cv_scores)))\n",
    "        \n",
    "    print(\"True Distribution:\")\n",
    "    print(pd.value_counts(target, normalize=True).sort_index())\n",
    "    print(\"Train Predicted Distribution:\")\n",
    "    print(pd.value_counts(pred_train, normalize=True).sort_index())\n",
    "    print(\"Test Predicted Distribution:\")\n",
    "    print(pd.value_counts(pred_test, normalize=True).sort_index())\n",
    "    \n",
    "    features_importances = pd.Series(model.feature_importances_, index=train.columns)\n",
    "    features_importances.nlargest(25).plot(kind='barh')\n",
    "    \n",
    "    return pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'https://d1kvfoyrif6wzg.cloudfront.net/assets/images/None/main/100_6771_3b0f897.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5c79a72b956d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_randomforest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-8e905f0b109d>\u001b[0m in \u001b[0;36mrun_randomforest_classifier\u001b[0;34m(train, test, label_column, scoring)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/anaconda3/envs/datascience-env/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'https://d1kvfoyrif6wzg.cloudfront.net/assets/images/None/main/100_6771_3b0f897.JPG'"
     ]
    }
   ],
   "source": [
    "pred_test = run_randomforest_classifier(train,test,\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TODO : \"LightGBM validation CV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 2\n",
    "pred_test = run_lgbm(train, test,'label',test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(train, test, label_column):\n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.fit_transform(test)\n",
    "    \n",
    "    svm_params = {'C': 1.0, \n",
    "                  'cache_size': 200, \n",
    "                  'class_weight': None, \n",
    "                  'coef0': 0.0, \n",
    "                  'decision_function_shape': 'ovr', \n",
    "                  'degree': 3, 'gamma': \n",
    "                  'auto_deprecated', \n",
    "                  'kernel': 'rbf', \n",
    "                  'max_iter': -1, \n",
    "                  'probability': False, \n",
    "                  'random_state': None, \n",
    "                  'shrinking': True, \n",
    "                  'tol': 0.001, \n",
    "                  'verbose': False}\n",
    "    \n",
    "    svc=SVC() \n",
    "    svc.fit(train_scaled,target)\n",
    "    y_pred_train=svc.predict(train_scaled)\n",
    "    score = accuracy_score(target,y_pred_train)\n",
    "    print('Accuracy Score: %.2f' % (score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_svm(train, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_voting_classifier(train, test, label_column):\n",
    "    \n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "    \n",
    "    ab_params = {'algorithm': 'SAMME.R', \n",
    "                 'base_estimator': None, \n",
    "                 'learning_rate': 0.1, \n",
    "                 'n_estimators': 20, \n",
    "                 'random_state': None}\n",
    "    \n",
    "    gbc_params = {'criterion': 'friedman_mse', \n",
    "                  'init': None, 'learning_rate': 0.1, \n",
    "                  'loss': 'deviance', \n",
    "                  'max_depth': 30, \n",
    "                  'max_features': None, \n",
    "                  'max_leaf_nodes': None, \n",
    "                  'min_impurity_decrease': 0.0, \n",
    "                  'min_impurity_split': None, \n",
    "                  'min_samples_leaf': 1, \n",
    "                  'min_samples_split': 2, \n",
    "                  'min_weight_fraction_leaf': 0.0, \n",
    "                  'n_estimators': 100, \n",
    "                  'n_iter_no_change': None, \n",
    "                  'presort': 'auto', \n",
    "                  'random_state': None, \n",
    "                  'subsample': 1.0, \n",
    "                  'tol': 0.0001, \n",
    "                  'validation_fraction': 0.1, \n",
    "                  'verbose': 0, \n",
    "                  'warm_start': False}\n",
    "    \n",
    "    bc_params = {'base_estimator': None, \n",
    "                 'bootstrap': True, \n",
    "                 'bootstrap_features': False, \n",
    "                 'max_features': 10, \n",
    "                 'max_samples': 1.0, \n",
    "                 'n_estimators': 20, \n",
    "                 'n_jobs': None, \n",
    "                 'oob_score': False, \n",
    "                 'random_state': None, \n",
    "                 'verbose': 0, \n",
    "                 'warm_start': False}\n",
    "    \n",
    "    clf1 = AdaBoostClassifier(**ab_params)\n",
    "    clf2 = GradientBoostingClassifier(**gbc_params)\n",
    "    clf3 = BaggingClassifier(**bc_params)\n",
    "    vote_clf = VotingClassifier(estimators=[('ab', clf1), ('gbc', clf2), ('bc', clf3)], weights=[0.2,1.7,0.6], voting='soft')\n",
    "    vote_clf = vote_clf.fit(train, target)\n",
    "    \n",
    "    pred_train = vote_clf.predict_proba(train)\n",
    "    pred_cv = cross_val_predict(vote_clf, train, np.ravel(target),\n",
    "                            method='predict_proba', cv=5, n_jobs=-1)\n",
    "    pred_test = vote_clf.predict_proba(test)\n",
    "    \n",
    "    print(\"LogLoss on train sample \", log_loss(y_pred=pred_train, y_true=target))\n",
    "    print(\"LogLoss on train sample (CV): \", log_loss(y_pred=pred_cv, y_true=target))\n",
    "    \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = run_voting_classifier(train, test, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_classifier(train, test, label_column):\n",
    "    \n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "\n",
    "    params = {'objective' : 'multi:softprob', \n",
    "              'num_class'  : 3,\n",
    "              'eval_metric' : 'mlogloss',\n",
    "              'nthread' : -1, \n",
    "              'booster' : \"gbtree\",\n",
    "              'gamma' : 0.01, \n",
    "              'max_depth' : 7,\n",
    "              'eta' : 0.1,\n",
    "              'min_child_weight'  : 0.7\n",
    "             }\n",
    "\n",
    "    clf_xgb = XGBClassifier(**params)\n",
    "\n",
    "    ppl = Pipeline([(\"clf\", clf_xgb)])\n",
    "\n",
    "    ppl.fit(train, np.ravel(y))\n",
    "\n",
    "    pred_train = ppl.predict_proba(train)\n",
    "    pred_cv = cross_val_predict(ppl, train, np.ravel(y),\n",
    "                                method='predict_proba', cv=5, n_jobs=-1)\n",
    "\n",
    "    print(\"LogLoss on train sample:\",log_loss(y_pred=pred_train, y_true=y))\n",
    "    print(\"LogLoss on train sample (CV):\",log_loss(y_pred=pred_cv, y_true=y))\n",
    "    \n",
    "    return pred_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0002766  -1.02109464 -1.00909881 -1.00265007 -1.00062673]\n",
      "RF CV mean : -1.01 \n",
      "RF CV std : 0.01 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "label_column = 'label'\n",
    "train_,target = split_target_and_df(train.dropna(axis=1),label_column)\n",
    "\n",
    "model = ExtraTreesClassifier(bootstrap=True , \n",
    "                                         criterion=\"gini\", \n",
    "                                         min_samples_leaf=10, \n",
    "                                         min_samples_split=100, \n",
    "                                         n_estimators=300,\n",
    "                                         random_state = 50,\n",
    "                                         n_jobs = -1)\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(model, train_, target, cv=5, scoring='neg_log_loss')\n",
    "print(cv_scores)\n",
    "print('RF CV mean : %.2f ' % (np.mean(cv_scores)))\n",
    "print('RF CV std : %.2f ' % (np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(pred_test, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"submission.csv\", index_label=\"id\", header=['0', '1', '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
